{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# As-a-Judge Methodology with Small LMs for Screening Studies in Systematic Reviews\n",
        "\n",
        "- This Colab saves all files to Google Drive but loads all data directly from GitHub. Therefore, you only need a Google Drive account to save the processed outputs.\n",
        "\n",
        "> ⚠️ **Attention:** To use this notebook, you will need your Hugging Face key (`hf_key`) and authorization from the model owners. Each model on Hugging Face has its own page with instructions on how to obtain usage permission.\n",
        "\n",
        "```python\n",
        "# Setup your Hugging Face key here\n",
        "hf_key = 'Your_HF_KEY'\n"
      ],
      "metadata": {
        "id": "54rUfRMbric9"
      },
      "id": "54rUfRMbric9"
    },
    {
      "cell_type": "code",
      "source": [
        "hf_key = 'YOU_KEY_HERE'"
      ],
      "metadata": {
        "id": "VUCQw0CYrg89"
      },
      "id": "VUCQw0CYrg89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f6da7761",
      "metadata": {
        "id": "f6da7761"
      },
      "source": [
        "## Import, Config and Some Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b384288d",
      "metadata": {
        "id": "b384288d"
      },
      "source": [
        "### Import main package's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9aaf725b",
      "metadata": {
        "id": "9aaf725b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3e92eb-6c5b-490a-9d6b-500e6b0cb0c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "It is running with GPU-T4\n"
          ]
        }
      ],
      "source": [
        "# show if GPU-T4 is running up\n",
        "!nvidia-smi\n",
        "print('It is running with GPU-T4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ee5021fe",
      "metadata": {
        "id": "ee5021fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c595d527-b668-41e7-afe0-7d8cb0ac3e7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main Packages Installed\n"
          ]
        }
      ],
      "source": [
        "# IMPORT MAIN PACKAGES\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# # mont drive for salve processed datasets\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import files\n",
        "from typing import List\n",
        "import os\n",
        "\n",
        "# update view config on pandas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# allow Rust use paraellel on python\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "print('Main Packages Installed')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cfe65a2",
      "metadata": {
        "id": "0cfe65a2"
      },
      "source": [
        "### Import custom packages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VbBn18_UUZUo",
      "metadata": {
        "id": "VbBn18_UUZUo"
      },
      "source": [
        "#### Balance dataframe with random_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xma7l1ZVUc1y",
      "metadata": {
        "id": "xma7l1ZVUc1y"
      },
      "outputs": [],
      "source": [
        "def balance_binary_df(df: pd.DataFrame, col: str, random_state=None):\n",
        "    \"\"\"\n",
        "    Balances a binary DataFrame by randomly selecting the same quantity\n",
        "    of the majority class, maintaining original indices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Original DataFrame.\n",
        "    col : str\n",
        "        Name of the binary column (string).\n",
        "    random_state : int, optional\n",
        "        Seed for reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df_low : DataFrame with the minority class\n",
        "    df_high : DataFrame with the complete majority class\n",
        "    df_random_high : Random sample of df_high with the same size as df_low\n",
        "    df_balanced : Union of df_low + df_random_high shuffled\n",
        "    \"\"\"\n",
        "\n",
        "    # Identify counts\n",
        "    value_counts = df[col].value_counts()\n",
        "\n",
        "    # Identify minority and majority class\n",
        "    low_class = value_counts.idxmin()\n",
        "    high_class = value_counts.idxmax()\n",
        "\n",
        "    # Separate dataframes\n",
        "    df_low = df[df[col] == low_class].copy()\n",
        "    df_high = df[df[col] == high_class].copy()\n",
        "\n",
        "    # Random sample of the majority class\n",
        "    df_random_high = df_high.sample(\n",
        "        n=len(df_low),\n",
        "        replace=False,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Merge maintaining indices and shuffling\n",
        "    df_balanced = pd.concat([df_low, df_random_high]).sample(\n",
        "        frac=1,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    return df_balanced\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SEu6OSt-URNi",
      "metadata": {
        "id": "SEu6OSt-URNi"
      },
      "source": [
        "#### Split Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pe0EfyT0US-K",
      "metadata": {
        "id": "pe0EfyT0US-K"
      },
      "outputs": [],
      "source": [
        "def split_dataframe(df: pd.DataFrame, chunk_size: int):\n",
        "    \"\"\"\n",
        "    Divides a DataFrame into fixed-size chunks,\n",
        "    preserving original indices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame to be divided.\n",
        "    chunk_size : int\n",
        "        Desired chunk size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[pd.DataFrame]\n",
        "        List of dataframes with up to chunk_size rows each.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    for start in range(0, len(df), chunk_size):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(df.iloc[start:end])\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a034050",
      "metadata": {
        "id": "6a034050"
      },
      "source": [
        "#### EC2 and EC8 clean-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d79164",
      "metadata": {
        "id": "f0d79164"
      },
      "outputs": [],
      "source": [
        "def drop_rows_with_ec2_or_ec8(df, applied_column='Applied Criterios'):\n",
        "    # if the 'applied_column' is not in the dataframe columns, raise an error\n",
        "    if applied_column not in df.columns:\n",
        "        raise ValueError(f\"column '{applied_column}' not found in the dataframe\")\n",
        "\n",
        "    def contains_ec2_or_ec8(value):\n",
        "        # if the value is NaN, return false\n",
        "        if pd.isna(value):\n",
        "            return False\n",
        "        # convert the value to uppercase for case-insensitive comparison\n",
        "        value_upper = str(value).upper()\n",
        "        # check if 'EC2' or 'EC8' is present in the value\n",
        "        return ('EC2' in value_upper) or ('EC8' in value_upper)\n",
        "\n",
        "    # filter the dataframe to exclude rows where the applied_column contains 'EC2' or 'EC8'\n",
        "    # reset the index of the filtered dataframe\n",
        "    df_filtered = df[~df[applied_column].apply(contains_ec2_or_ec8)].reset_index(drop=True)\n",
        "    return df_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92ec24ff",
      "metadata": {
        "id": "92ec24ff"
      },
      "source": [
        "#### Shadow Clone and Drop Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f772072b",
      "metadata": {
        "id": "f772072b"
      },
      "outputs": [],
      "source": [
        "def shadow_clone_and_drop_no_target_columns(df, columns):\n",
        "    df2 = df.copy(deep=True)\n",
        "    df2 = df2.drop(columns=columns)\n",
        "    return df2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea70117f",
      "metadata": {
        "id": "ea70117f"
      },
      "source": [
        "#### Normalize Status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcb6508a",
      "metadata": {
        "id": "bcb6508a"
      },
      "outputs": [],
      "source": [
        "def normalize_status(value: str):\n",
        "    if isinstance(value, str):\n",
        "        v = value.strip()\n",
        "\n",
        "        if v.lower() == \"incluidos\":\n",
        "            return \"Included\"\n",
        "\n",
        "        if v.lower().startswith(\"e\"):\n",
        "            return \"Excluded\"\n",
        "\n",
        "    return value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cd6ae29",
      "metadata": {
        "id": "3cd6ae29"
      },
      "source": [
        "### Import original dataset - from github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "564f4872",
      "metadata": {
        "id": "564f4872",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7776723-02b4-4383-b604-00a9dbfe6988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imported from cache\n",
            "Index(['Revisores', 'ID', 'CODE', 'STATUS', 'Applied Criterios', 'REASON',\n",
            "       'CATEGORY', 'SUB-CATEGORY', 'YEAR', 'COUNTRY', 'SOURCE', 'TITLE',\n",
            "       'AUTHORS', 'ABSTRACT', 'URL', 'Unnamed: 15'],\n",
            "      dtype='object')\n",
            "imported\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# import dataset and cache files\n",
        "url  = \"https://github.com/izichtl/small-language-models-on-systematic-reviews/raw/main/MEDEIROS_2015_ORIGINAL_RSL_FILES/medeiros_2015_original_rsl_state_01_result.xls\"\n",
        "filename = \"medeiros_2015_original_rsl_state_01_result.xls\"\n",
        "\n",
        "# cache the file\n",
        "if not os.path.exists(filename):\n",
        "    print('imported from url')\n",
        "    df = pd.read_excel(url)\n",
        "    df.to_csv(filename, index=False)\n",
        "else:\n",
        "    print('imported from cache')\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "print(df.columns)\n",
        "print('imported')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12bd73ce",
      "metadata": {
        "id": "12bd73ce"
      },
      "source": [
        "## Pre-Processamento\n",
        "\n",
        "- Needs import, config and sf."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad8d5a0d",
      "metadata": {
        "id": "ad8d5a0d"
      },
      "source": [
        "### Understand dataset and look up for columns with nulls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26bc41be",
      "metadata": {
        "id": "26bc41be"
      },
      "outputs": [],
      "source": [
        "print(df.info())\n",
        "(df.isnull().mean() * 100).sort_values(ascending=False)\n",
        "df[\"STATUS\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93989685",
      "metadata": {
        "id": "93989685"
      },
      "source": [
        "### Cleaning the columns is not relevant to the experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fce481a",
      "metadata": {
        "id": "6fce481a"
      },
      "outputs": [],
      "source": [
        "# clean-up and clone df to work df\n",
        "rsl_df = shadow_clone_and_drop_no_target_columns(df, ['Revisores',  'CODE', 'REASON', 'CATEGORY', 'SUB-CATEGORY', 'COUNTRY', 'SOURCE', 'AUTHORS', 'URL', 'YEAR', 'Unnamed: 15'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2215420a",
      "metadata": {
        "id": "2215420a"
      },
      "source": [
        "### Cleaning the instances with EC2 and EC8, these criteria cannot be decided by the LLM.\n",
        "\n",
        "- EC2. Estudos duplicados ou repetidos.\n",
        "- EC8. Artigos que não estão disponíveis gratuitamente para download nos ambientes institucionais do CIn/UFPE ou do IFPB;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c152f49",
      "metadata": {
        "id": "8c152f49"
      },
      "outputs": [],
      "source": [
        "rsl_df = drop_rows_with_ec2_or_ec8(rsl_df)\n",
        "\n",
        "# show nulls and info after critearia clean up\n",
        "# print(rsl_df.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5713099",
      "metadata": {
        "id": "c5713099"
      },
      "source": [
        "### Cleaning instances with a null abstract or title, because the LLM uses both to make decisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5368f73e",
      "metadata": {
        "id": "5368f73e"
      },
      "outputs": [],
      "source": [
        "rsl_df = rsl_df.dropna(subset=[\"ABSTRACT\"])\n",
        "rsl_df = rsl_df.dropna(subset=[\"TITLE\"])\n",
        "\n",
        "print(rsl_df.loc[rsl_df[\"ABSTRACT\"].isnull(), [\"ID\", \"TITLE\", \"ABSTRACT\"]])\n",
        "print(rsl_df.loc[rsl_df[\"TITLE\"].isnull(), [\"ID\", \"ABSTRACT\", \"ABSTRACT\"]])\n",
        "\n",
        "# show nulls and info after critearia clean up\n",
        "print(rsl_df.info())\n",
        "(rsl_df.isnull().mean() * 100).sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de2f6ba8",
      "metadata": {
        "id": "de2f6ba8"
      },
      "source": [
        "### Normalization of the Status and Applied Criteria columns to better understand the pre-processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d692211",
      "metadata": {
        "id": "3d692211"
      },
      "outputs": [],
      "source": [
        "\n",
        "# describes status column\n",
        "print('Absolut Value')\n",
        "print(rsl_df[\"STATUS\"].value_counts())\n",
        "print('Percent')\n",
        "print(rsl_df[\"STATUS\"].value_counts(normalize=True) * 100)\n",
        "print('')\n",
        "\n",
        "print(rsl_df[\"Applied Criterios\"].value_counts())\n",
        "rsl_df[\"Applied Criterios\"] = rsl_df[\"Applied Criterios\"].str.strip()\n",
        "\n",
        "print('')\n",
        "# to confirm, where where the status is \"included\" and the criteria is NaN\n",
        "\n",
        "count = rsl_df[\n",
        "    (rsl_df[\"STATUS\"] == \"Included\") &\n",
        "    (rsl_df[\"Applied Criterios\"].isna())\n",
        "].shape[0]\n",
        "# print(count)\n",
        "\n",
        "# transfor Applied Criterios where is NaN to \"included\"\n",
        "rsl_df[\"Applied Criterios\"] = np.where(rsl_df[\"Applied Criterios\"].isna(), \"included\", rsl_df[\"Applied Criterios\"])\n",
        "print('')\n",
        "\n",
        "print(rsl_df[\"Applied Criterios\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xM6D04lEQOvc",
      "metadata": {
        "id": "xM6D04lEQOvc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "KuRZw0QEQRcM",
      "metadata": {
        "id": "KuRZw0QEQRcM"
      },
      "source": [
        "### Balancing data to 50% included and 50% excluded\n",
        "\n",
        "Each entry takes 60 - 90 seconds to be processed into model A and B, so a reduce and balance data is a logical choice to turn de experiment be run in time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6NfhYRQWRCze",
      "metadata": {
        "collapsed": true,
        "id": "6NfhYRQWRCze"
      },
      "outputs": [],
      "source": [
        "# balanced with random state to be exact result between experiments\n",
        "df_balanced_a = balance_binary_df(rsl_df, col='STATUS', random_state=42)\n",
        "df_balanced_b = balance_binary_df(rsl_df, col='STATUS', random_state=42)\n",
        "\n",
        "if df_balanced_a.equals(df_balanced_b):\n",
        "    print(\"dataframes are equals\")\n",
        "\n",
        "df_balanced = df_balanced_a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a65ac3db",
      "metadata": {
        "id": "a65ac3db"
      },
      "source": [
        "### Saving RSL pre-processed-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c35d1ce",
      "metadata": {
        "id": "4c35d1ce"
      },
      "outputs": [],
      "source": [
        "# df_balanced.to_csv(\"rsl-pre-processed-dataset-balanced.csv\", index=False)\n",
        "# files.download(\"rsl-pre-processed-dataset-balanced.csv\")\n",
        "# print(df_balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gn5cfuEwUxM8",
      "metadata": {
        "id": "Gn5cfuEwUxM8"
      },
      "source": [
        "### Split dataframe by 100 entrys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vQIAL3UQVEOB",
      "metadata": {
        "id": "vQIAL3UQVEOB"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_in_parts = split_dataframe(df_balanced.sample(frac=1, random_state=42), 100)\n",
        "print(len(df_in_parts))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5ba200a",
      "metadata": {
        "id": "f5ba200a"
      },
      "source": [
        "## Evaluators\n",
        "\n",
        "- needs hf key and can run without other code cells"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "817864e2",
      "metadata": {
        "id": "817864e2"
      },
      "source": [
        "#### Evaluator models and Hugginface packages import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4611fc",
      "metadata": {
        "id": "ab4611fc"
      },
      "outputs": [],
      "source": [
        "%pip install -q transformers accelerate\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from huggingface_hub import login\n",
        "login(hf_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "COl9GJBzQ7Hh",
      "metadata": {
        "id": "COl9GJBzQ7Hh"
      },
      "outputs": [],
      "source": [
        "# evaluators control\n",
        "ENABLE_PIPE_A = False\n",
        "ENABLE_PIPE_B = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d2a32ed",
      "metadata": {
        "id": "8d2a32ed"
      },
      "source": [
        "#### Evaluator A \"Qwen/Qwen3-4B-Instruct-2507\" loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3b623d",
      "metadata": {
        "id": "fb3b623d"
      },
      "outputs": [],
      "source": [
        "if ENABLE_PIPE_A:\n",
        "    classification_text_task_pipe_A = pipeline(\n",
        "        task=\"text-generation\",\n",
        "        model=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
        "        device=0,\n",
        "        return_full_text=False,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bfcb0f5",
      "metadata": {
        "id": "5bfcb0f5"
      },
      "source": [
        "#### Evaluator B \"google/gemma-3-4b-it\" loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2957ed",
      "metadata": {
        "id": "1d2957ed"
      },
      "outputs": [],
      "source": [
        "if ENABLE_PIPE_B:\n",
        "  classification_text_task_pipe_B = pipeline(\n",
        "      task=\"text-generation\",\n",
        "      model=\"google/gemma-3-4b-it\",\n",
        "      device=0,\n",
        "      return_full_text=False,\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9721537",
      "metadata": {
        "id": "b9721537"
      },
      "source": [
        "#### Evaluator Pipeline Functions and Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c24bc98b",
      "metadata": {
        "id": "c24bc98b"
      },
      "outputs": [],
      "source": [
        "def extract_fields(text):\n",
        "    result_match = re.search(r'\"result\"\\s*:\\s*\"([^\"]*)\"', text)\n",
        "    explanation_match = re.search(r'\"explanation\"\\s*:\\s*\"([^\"]*)\"', text)\n",
        "\n",
        "    return {\n",
        "        \"result\": result_match.group(1) if result_match else None,\n",
        "        \"explanation\": explanation_match.group(1) if explanation_match else None\n",
        "    }\n",
        "\n",
        "def avaliador_execute_pipeline(\n",
        "    df, target_cols, prompt_template, pipe,\n",
        "    output_col_result=\"result\",\n",
        "    output_col_explain=\"explain\",\n",
        "    max_tokens=256\n",
        "):\n",
        "    df2 = df.copy(deep=True)\n",
        "\n",
        "    def safe_str(x):\n",
        "        return \"\" if pd.isna(x) else str(x)\n",
        "\n",
        "    resultados = []\n",
        "    explanations = []\n",
        "\n",
        "    for i, (idx, row) in enumerate(df2.iterrows()):\n",
        "        prompt = prompt_template.format(*[safe_str(row[c]) for c in target_cols])\n",
        "        print(\"=== INSTANCE ===\")\n",
        "        print(i)\n",
        "\n",
        "        output = pipe(prompt)\n",
        "\n",
        "        print(\"=== RESPONSE ===\")\n",
        "        # print(\"Raw output:\", output)\n",
        "        print('------------------')\n",
        "\n",
        "        s = output[0]['generated_text']\n",
        "\n",
        "        # ----- FIELD EXTRACTION -----\n",
        "        parsed = extract_fields(s)\n",
        "\n",
        "        # Check if both were found\n",
        "        if parsed[\"result\"] is not None and parsed[\"explanation\"] is not None:\n",
        "            resultados.append(parsed[\"result\"])\n",
        "            explanations.append(parsed[\"explanation\"])\n",
        "        else:\n",
        "            # If it fails, put the raw string in result and an empty explanation\n",
        "            resultados.append(s)\n",
        "            explanations.append(\"\")\n",
        "\n",
        "        print(parsed)\n",
        "        print('------------------')\n",
        "        print(\"=== RESPONSE-END ===\")\n",
        "\n",
        "    df2[output_col_result] = resultados\n",
        "    df2[output_col_explain] = explanations\n",
        "\n",
        "    return df2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cf0e598",
      "metadata": {
        "id": "0cf0e598"
      },
      "source": [
        "#### Evaluator Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c668940c",
      "metadata": {
        "id": "c668940c"
      },
      "source": [
        "##### Prompt v2 - CoT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "978d0dac",
      "metadata": {
        "id": "978d0dac"
      },
      "outputs": [],
      "source": [
        "evaluator_prompt_v2 = \"\"\"\"\n",
        "You are a scientific article evaluator. Classify the following article as INCLUDED or EXCLUDED\n",
        "based on the provided inclusion and exclusion criteria. After classifying, provide a brief explanation\n",
        "justifying your decision.\n",
        "\n",
        "To perform the evaluation, follow this step-by-step reasoning:\n",
        "\n",
        "1. Check whether the article violates any Exclusion Criteria:\n",
        "• EC1. Is the article written in a language other than English?\n",
        "• EC3. Does the study not address software requirements?\n",
        "• EC4. Is the study incomplete, a draft, presentation slides, or just an abstract?\n",
        "• EC5. Is the study tertiary or a meta-analysis?\n",
        "• EC6. Does it deal with teaching agile methods?\n",
        "• EC7. Does it fail to address at least one agile methodology?\n",
        "• EC9. Does it fail to answer at least one research question?\n",
        "\n",
        "2. Confirm whether the article meets all Inclusion Criteria:\n",
        "• IC1. Does it address requirements in software projects using agile methodologies?\n",
        "• IC2. Is it a study from industry or academia?\n",
        "• IC3. Is it qualitative or quantitative research?\n",
        "• IC4. Is it a primary or secondary study?\n",
        "\n",
        "3. If the article does not violate any Exclusion Criteria and satisfies the Inclusion Criteria, classify it as \"INCLUDED\". Otherwise, classify it as \"EXCLUDED\".\n",
        "\n",
        "4. Provide a brief explanation that references the applied criteria justifying your classification.\n",
        "\n",
        "---\n",
        "Title: {0}\n",
        "-\n",
        "Abstract: {1}\n",
        "\n",
        "Instruction:\n",
        "Respond ONLY with a single valid JSON object in the following format, without any additional text or code blocks:\n",
        "\n",
        "{{\n",
        "  \"result\": \"INCLUDED\" or \"EXCLUDED\",\n",
        "  \"explanation\": \"<brief and clear explanation of your classification>\"\n",
        "}}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e71b8ecf",
      "metadata": {
        "id": "e71b8ecf"
      },
      "source": [
        "#### Execution Evaluator A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rApSA6vtOsLH",
      "metadata": {
        "id": "rApSA6vtOsLH"
      },
      "outputs": [],
      "source": [
        "# define the evaluator variable\n",
        "# check if this is the expected execution\n",
        "ITEM = \"A\"\n",
        "EXECUTION = '3'\n",
        "if ENABLE_PIPE_A:\n",
        "\n",
        "    # # google drive directory\n",
        "    # output_dir = \"/content/drive/MyDrive/PLN_DATASETS\"\n",
        "\n",
        "    # # ensure the folder exists\n",
        "    # os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(\"total chunks:\", len(df_in_parts))\n",
        "\n",
        "    # loop through all chunks\n",
        "    for i, df_input in enumerate(df_in_parts, start=1):\n",
        "        print(f\"\\n processing chunk {i}/{len(df_in_parts)}...\")\n",
        "        print(\"size:\", len(df_input))\n",
        "\n",
        "        # use variable b dynamically in the output columns\n",
        "        result_col = f\"RESULT_{ITEM}_{EXECUTION}\"\n",
        "        explain_col = f\"EXPLAIN_{ITEM}_{EXECUTION}\"\n",
        "\n",
        "        df_result = avaliador_execute_pipeline(\n",
        "            df=df_input,\n",
        "            target_cols=[\"TITLE\", \"ABSTRACT\"],\n",
        "            prompt_template=evaluator_prompt_v2,\n",
        "            pipe=classification_text_task_pipe_A,\n",
        "            output_col_result=result_col,\n",
        "            output_col_explain=explain_col,\n",
        "        )\n",
        "\n",
        "        # automatic filename based on variable b\n",
        "        output_filename = f\"rsl-pre-processed-dataset_{ITEM.lower()}_{i}_execution_{EXECUTION}.csv\"\n",
        "        output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        # # save to google drive\n",
        "        # df_result.to_csv(output_path, index=False)\n",
        "        # print(f\"✔ file saved: {output_path}\")\n",
        "\n",
        "    print(\"\\n done with successsss.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f11c5a",
      "metadata": {
        "id": "75f11c5a"
      },
      "source": [
        "#### Execution Evaluator B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575d0a2f",
      "metadata": {
        "id": "575d0a2f"
      },
      "outputs": [],
      "source": [
        "# define the evaluator variable b\n",
        "# check if this is the expected execution\n",
        "ITEM = \"B\"\n",
        "EXECUTION = '1'\n",
        "if ENABLE_PIPE_B:\n",
        "\n",
        "    # # google drive directory\n",
        "    # output_dir = \"/content/drive/MyDrive/PLN_DATASETS\"\n",
        "\n",
        "    # # ensure the folder exists\n",
        "    # os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(\"total chunks:\", len(df_in_parts))\n",
        "\n",
        "    # loop through all chunks\n",
        "    for i, df_input in enumerate(df_in_parts, start=1):\n",
        "        print(f\"\\n processing chunk {i}/{len(df_in_parts)}...\")\n",
        "        print(\"size:\", len(df_input))\n",
        "\n",
        "        # use variable b dynamically in the output columns\n",
        "        result_col = f\"RESULT_{ITEM}_{EXECUTION}\"\n",
        "        explain_col = f\"EXPLAIN_{ITEM}_{EXECUTION}\"\n",
        "\n",
        "        df_result = avaliador_execute_pipeline(\n",
        "            df=df_input,\n",
        "            target_cols=[\"TITLE\", \"ABSTRACT\"],\n",
        "            prompt_template=evaluator_prompt_v2,\n",
        "            pipe=evaluator_prompt_v2,\n",
        "            output_col_result=result_col,\n",
        "            output_col_explain=explain_col,\n",
        "        )\n",
        "\n",
        "        # automatic filename based on variable b\n",
        "        output_filename = f\"rsl-pre-processed-dataset_{ITEM.lower()}_{i}_execution_{EXECUTION}.csv\"\n",
        "        output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        # # save to google drive\n",
        "        # df_result.to_csv(output_path, index=False)\n",
        "        # print(f\"✔ file saved: {output_path}\")\n",
        "\n",
        "    print(\"\\n done with successsss.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AM6WNnE_5tuq",
      "metadata": {
        "id": "AM6WNnE_5tuq"
      },
      "source": [
        "## Post-processing\n",
        "\n",
        "- can run withouu other cells"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98YADYlOC2Ak",
      "metadata": {
        "id": "98YADYlOC2Ak"
      },
      "source": [
        "### Imports packages used in this section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oom35b75C6UV",
      "metadata": {
        "id": "Oom35b75C6UV"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# reforce Rust parallel import\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6hN536hIhEV0"
      },
      "id": "6hN536hIhEV0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Judged Datasets from github"
      ],
      "metadata": {
        "id": "vscs7v6Mgyrg"
      },
      "id": "vscs7v6Mgyrg"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RtwOoreSgyrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935543f1-8e3d-4efa-90b8-a0017ac7788d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "judged dataset a length: 598\n",
            "judged dataset b length: 598\n",
            "judged dataset c length: 598\n"
          ]
        }
      ],
      "source": [
        "judged_a_url = 'https://raw.githubusercontent.com/izichtl/small-language-models-on-systematic-reviews/refs/heads/main/PLN_JUDGED_DATASETS/rsl-judge-processed-dataset_a_execution_1.csv'\n",
        "judged_b_url = 'https://raw.githubusercontent.com/izichtl/small-language-models-on-systematic-reviews/refs/heads/main/PLN_JUDGED_DATASETS/rsl-judge-processed-dataset_b_execution_1.csv'\n",
        "judged_c_url = 'https://raw.githubusercontent.com/izichtl/small-language-models-on-systematic-reviews/refs/heads/main/PLN_JUDGED_DATASETS/rsl-judge-processed-dataset_c_execution_1.csv'\n",
        "\n",
        "judged_a_filename = 'rsl-judge-processed-dataset_a_execution_1.csv'\n",
        "judged_b_filename = 'rsl-judge-processed-dataset_b_execution_1.csv'\n",
        "judged_c_filename = 'rsl-judge-processed-dataset_c_execution_1.csv'\n",
        "\n",
        "if not os.path.exists(judged_a_filename):\n",
        "    df_judged_a = pd.read_csv(judged_a_url)\n",
        "    df_judged_a.to_csv(judged_a_filename, index=False)\n",
        "else:\n",
        "    df_judged_a = pd.read_csv(judged_a_filename)\n",
        "\n",
        "if not os.path.exists(judged_b_filename):\n",
        "    df_judged_b = pd.read_csv(judged_b_url)\n",
        "    df_judged_b.to_csv(judged_b_filename, index=False)\n",
        "else:\n",
        "    df_judged_b = pd.read_csv(judged_b_filename)\n",
        "\n",
        "if not os.path.exists(judged_c_filename):\n",
        "    df_judged_c = pd.read_csv(judged_c_url)\n",
        "    df_judged_c.to_csv(judged_c_filename, index=False)\n",
        "else:\n",
        "    df_judged_c = pd.read_csv(judged_c_filename)\n",
        "\n",
        "print(f\"judged dataset a length: {len(df_judged_a)}\")\n",
        "print(f\"judged dataset b length: {len(df_judged_b)}\")\n",
        "print(f\"judged dataset c length: {len(df_judged_c)}\")\n"
      ],
      "id": "RtwOoreSgyrh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To normalize judge response"
      ],
      "metadata": {
        "id": "wQ6k54fcgyrh"
      },
      "id": "wQ6k54fcgyrh"
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_judge_labels(df, target_column):\n",
        "    \"\"\"\n",
        "    standardizes the judgment column labels.\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    valid_labels = ['INCLUDED', 'EXCLUDED']\n",
        "\n",
        "    non_standard_mask = ~df_processed[target_column].isin(valid_labels)\n",
        "\n",
        "    if non_standard_mask.sum() > 0:\n",
        "        df_processed.loc[non_standard_mask, target_column] = 'INCLUDED'\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "\n",
        "df_judged_a = standardize_judge_labels(df_judged_a, 'RESULT_JUDGE_A')\n",
        "df_judged_b = standardize_judge_labels(df_judged_b, 'RESULT_JUDGE_B')\n",
        "df_judged_c = standardize_judge_labels(df_judged_c, 'RESULT_JUDGE_C')\n"
      ],
      "metadata": {
        "id": "WVAsN29ngyrh"
      },
      "execution_count": null,
      "outputs": [],
      "id": "WVAsN29ngyrh"
    },
    {
      "cell_type": "code",
      "source": [
        "judge_final_merged_to_metric = df_judged_a.merge(\n",
        "    df_judged_b[[\"ID\", \"RESULT_JUDGE_B\", \"EXPLAIN_JUDGEB\"]],\n",
        "    on=\"ID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "judge_final_merged_to_metric = judge_final_merged_to_metric.merge(\n",
        "    df_judged_c[[\"ID\", \"RESULT_JUDGE_C\", \"EXPLAIN_JUDGEC\"]],\n",
        "    on=\"ID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# judge_final_merged_to_metric\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5194gFiGgyrh"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5194gFiGgyrh"
    },
    {
      "cell_type": "markdown",
      "id": "vFwmdiJbOqiY",
      "metadata": {
        "id": "vFwmdiJbOqiY"
      },
      "source": [
        "### Import of Evaluated Dataset from github."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "Zz2mUB7t1Pyo",
      "metadata": {
        "id": "Zz2mUB7t1Pyo"
      },
      "outputs": [],
      "source": [
        "base_evaluated_url = \"https://raw.githubusercontent.com/izichtl/small-language-models-on-systematic-reviews/refs/heads/main/PLN_EVALUATED_DATASETS\"\n",
        "\n",
        "# import a list of files of one execution of evaluator\n",
        "def get_execution_datasets(base_url, execution):\n",
        "    \"\"\"\n",
        "    return a 6 item chunk list.\n",
        "    \"\"\"\n",
        "    datasets = []\n",
        "\n",
        "    cache_dir = f\"cache_{execution[0]}_{execution[1]}\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    model_name = execution[4] if len(execution) > 4 else \"unknown\"\n",
        "    print(f\"processing: {model_name} - {execution[1]}\")\n",
        "    print(\"-----------------------\")\n",
        "\n",
        "    for item in [1, 2, 3, 4, 5, 6]:\n",
        "        url = f\"{base_url}/{execution[0]}/{execution[1]}/rsl-pre-processed-dataset_{execution[2]}_{item}_execution_{execution[3]}.csv\"\n",
        "        file_name = f\"rsl-pre-processed-dataset_{execution[2]}_{item}_execution_{execution[3]}.csv\"\n",
        "        local_path = os.path.join(cache_dir, file_name)\n",
        "\n",
        "        try:\n",
        "            if os.path.exists(local_path):\n",
        "                df = pd.read_csv(local_path)\n",
        "            else:\n",
        "                df = pd.read_csv(url)\n",
        "                df.to_csv(local_path, index=False)\n",
        "\n",
        "            datasets.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"error getting {url}: {e}\")\n",
        "\n",
        "    return datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7bcUWXQ0-pJ4",
      "metadata": {
        "id": "7bcUWXQ0-pJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a73470-1362-48b4-ef2c-4ff5650362d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing: qwen - EXECUTION_1\n",
            "-----------------------\n",
            "processing: qwen - EXECUTION_2\n",
            "-----------------------\n",
            "processing: qwen - EXECUTION_3\n",
            "-----------------------\n",
            "processing: gemma - EXECUTION_1\n",
            "-----------------------\n",
            "processing: gemma - EXECUTION_2\n",
            "-----------------------\n",
            "processing: gemma - EXECUTION_3\n",
            "-----------------------\n"
          ]
        }
      ],
      "source": [
        "# Folder and group maps\n",
        "evaludated_a_folders = [\n",
        "    [\"EVALUATED_A\",\"EXECUTION_1\", \"a\", \"1\", \"qwen\"],\n",
        "    [\"EVALUATED_A\",\"EXECUTION_2\", \"a\", \"2\", \"qwen\"],\n",
        "    [\"EVALUATED_A\",\"EXECUTION_3\", \"a\", \"3\", \"qwen\"],\n",
        "]\n",
        "\n",
        "evaludated_b_folders = [\n",
        "    [\"EVALUATED_B\", \"EXECUTION_1\", \"b\", \"1\", \"gemma\"],\n",
        "    [\"EVALUATED_B\", \"EXECUTION_2\", \"b\", \"2\", \"gemma\"],\n",
        "    [\"EVALUATED_B\", \"EXECUTION_3\", \"b\", \"3\", \"gemma\"],\n",
        "]\n",
        "\n",
        "# A\n",
        "evaluator_a_execution_1 = evaludated_a_folders[0]\n",
        "evaluator_a_execution_1 = get_execution_datasets(base_evaluated_url, evaluator_a_execution_1)\n",
        "evaluator_a_execution_1 = pd.concat(evaluator_a_execution_1, ignore_index=True)\n",
        "\n",
        "evaluator_a_execution_2 = evaludated_a_folders[1]\n",
        "evaluator_a_execution_2 = get_execution_datasets(base_evaluated_url, evaluator_a_execution_2)\n",
        "evaluator_a_execution_2 = pd.concat(evaluator_a_execution_2, ignore_index=True)\n",
        "\n",
        "evaluator_a_execution_3 = evaludated_a_folders[2]\n",
        "evaluator_a_execution_3 = get_execution_datasets(base_evaluated_url, evaluator_a_execution_3)\n",
        "evaluator_a_execution_3 = pd.concat(evaluator_a_execution_3, ignore_index=True)\n",
        "\n",
        "# B\n",
        "evaluator_b_execution_1 = evaludated_b_folders[0]\n",
        "evaluator_b_execution_1 = get_execution_datasets(base_evaluated_url, evaluator_b_execution_1)\n",
        "evaluator_b_execution_1 = pd.concat(evaluator_b_execution_1, ignore_index=True)\n",
        "\n",
        "evaluator_b_execution_2 = evaludated_b_folders[1]\n",
        "evaluator_b_execution_2 = get_execution_datasets(base_evaluated_url, evaluator_b_execution_2)\n",
        "evaluator_b_execution_2 = pd.concat(evaluator_b_execution_2, ignore_index=True)\n",
        "\n",
        "evaluator_b_execution_3 = evaludated_b_folders[2]\n",
        "evaluator_b_execution_3 = get_execution_datasets(base_evaluated_url, evaluator_b_execution_3)\n",
        "evaluator_b_execution_3 = pd.concat(evaluator_b_execution_3, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UEhIMopugykI"
      },
      "id": "UEhIMopugykI"
    },
    {
      "cell_type": "markdown",
      "id": "UIPjyTlW50GX",
      "metadata": {
        "id": "UIPjyTlW50GX"
      },
      "source": [
        "### Looking for bad formation json's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v3BJZhJ0AFlG",
      "metadata": {
        "id": "v3BJZhJ0AFlG"
      },
      "outputs": [],
      "source": [
        "# Looking for bad formation json's\n",
        "\n",
        "# A\n",
        "evaluator_a_execution_1_filted = ~evaluator_a_execution_1['RESULT_A_1'].isin(['INCLUDED', 'EXCLUDED'])\n",
        "print('Instances on group_a/_1 that require post-processing')\n",
        "print(len(evaluator_a_execution_1_filted) - len(evaluator_a_execution_1))\n",
        "\n",
        "evaluator_a_execution_2_filted = ~evaluator_a_execution_2['RESULT_A_2'].isin(['INCLUDED', 'EXCLUDED'])\n",
        "print('Instances on group_a/_3 that require post-processing')\n",
        "print(len(evaluator_a_execution_2_filted) - len(evaluator_a_execution_2))\n",
        "\n",
        "evaluator_a_execution_3_filted = ~evaluator_a_execution_3['RESULT_A_3'].isin(['INCLUDED', 'EXCLUDED'])\n",
        "print('Instances on group_a/_3 that require post-processing')\n",
        "print(len(evaluator_a_execution_3_filted) - len(evaluator_a_execution_3))\n",
        "\n",
        "# B\n",
        "evaluator_b_execution_1_filted = ~evaluator_b_execution_1['RESULT_B_1'].isin(['INCLUDED', 'EXCLUDED'])\n",
        "print('Instances on group_b/_1 that require post-processing')\n",
        "print(len(evaluator_b_execution_1_filted) - len(evaluator_b_execution_1))\n",
        "\n",
        "evaluator_b_execution_2_filted = ~evaluator_b_execution_2['RESULT_B_2'].isin(['INCLUDED', 'EXCLUDED'])\n",
        "print('Instances on group_b/_2 that require post-processing')\n",
        "print(len(evaluator_b_execution_2_filted) - len(evaluator_b_execution_2))\n",
        "\n",
        "evaluator_b_execution_3_filted = ~evaluator_b_execution_3['RESULT_B_3'].isin(['INCLUDED', 'EXCLUDED'])\n",
        "print('Instances on group_b/_3 that require post-processing')\n",
        "print(len(evaluator_b_execution_2_filted) - len(evaluator_b_execution_3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vrYw9iNHGmdN",
      "metadata": {
        "id": "vrYw9iNHGmdN"
      },
      "source": [
        "### Merging into on dataset by group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8qSpoR1uGl_f",
      "metadata": {
        "id": "8qSpoR1uGl_f"
      },
      "outputs": [],
      "source": [
        "# Merging Group A\n",
        "df_evaluated_A_merged = evaluator_a_execution_1.merge(\n",
        "    evaluator_a_execution_2[['ID', 'RESULT_A_2', 'EXPLAIN_A_2']],\n",
        "    on='ID',\n",
        "    how='left'\n",
        ")\n",
        "df_evaluated_A_merged = df_evaluated_A_merged.merge(\n",
        "    evaluator_a_execution_3[['ID', 'RESULT_A_3', 'EXPLAIN_A_3']],\n",
        "    on='ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Merging Group B\n",
        "df_evaluated_B_merged = evaluator_b_execution_1.merge(\n",
        "    evaluator_b_execution_2[['ID', 'RESULT_B_2', 'EXPLAIN_B_2']],\n",
        "    on='ID',\n",
        "    how='left'\n",
        ")\n",
        "df_evaluated_B_merged = df_evaluated_B_merged.merge(\n",
        "    evaluator_b_execution_3[['ID', 'RESULT_B_3', 'EXPLAIN_B_3']],\n",
        "    on='ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(\"Evaluator A merged into df_evaluated_A_merged \")\n",
        "print(\"Evaluator B merged into df_evaluated_B_merged \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vcPDXceTCiP8",
      "metadata": {
        "id": "vcPDXceTCiP8"
      },
      "source": [
        "### Method of Majority Voting Semantic Consensus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8IkRWHnGCkeK",
      "metadata": {
        "id": "8IkRWHnGCkeK"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def get_best_explication_by_semantic_sim(explicacoes):\n",
        "    \"\"\"\n",
        "    Receives a list of explanations, filters invalid values, and returns the most representative one.\n",
        "    \"\"\"\n",
        "    # --- PROTECTION 1: Filter NaNs, None, and empty strings ---\n",
        "    valid_explanations = [\n",
        "        str(e).strip() for e in explicacoes\n",
        "        if pd.notna(e) and isinstance(e, str) and len(str(e).strip()) > 0\n",
        "    ]\n",
        "\n",
        "    # --- PROTECTION 2: Check if anything remains ---\n",
        "    if not valid_explanations:\n",
        "        return \"No valid explanation available.\"\n",
        "\n",
        "    if len(valid_explanations) == 1:\n",
        "        return valid_explanations[0]\n",
        "\n",
        "    try:\n",
        "        # Calculate embeddings only for valid strings\n",
        "        emb = model.encode(valid_explanations)\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        sim = cosine_similarity(emb)\n",
        "\n",
        "        # Sum scores to find the semantic \"center\"\n",
        "        scores = sim.sum(axis=1)\n",
        "        idx = np.argmax(scores)\n",
        "\n",
        "        return valid_explanations[idx]\n",
        "\n",
        "    except Exception as e:\n",
        "        # Safety fallback if the model fails\n",
        "        return valid_explanations[0]\n",
        "\n",
        "\n",
        "def apply_majority_voting(\n",
        "    df: pd.DataFrame,\n",
        "    cols_result: list,\n",
        "    cols_explain: list,\n",
        "    binary_options: list,\n",
        "    output_result_col: str,\n",
        "    output_explain_col: str\n",
        "):\n",
        "    final_results = []\n",
        "    final_explanations = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Capture results and explanations from the row\n",
        "        results = [row[c] for c in cols_result]\n",
        "        explains = [row[c] for c in cols_explain]\n",
        "\n",
        "        # --- 1. Determine the majority class ---\n",
        "        # Filter NaNs in the results so as not to count null votes as a class\n",
        "        valid_results = [r for r in results if pd.notna(r) and r in binary_options]\n",
        "\n",
        "        if not valid_results:\n",
        "            # Extreme case: all models failed/returned NaN\n",
        "            final_results.append(\"UNDETERMINED\")\n",
        "            final_explanations.append(\"No valid results for voting.\")\n",
        "            continue\n",
        "\n",
        "        cont = Counter(valid_results)\n",
        "        # Get the most common class\n",
        "        major_class = cont.most_common(1)[0][0]\n",
        "        final_results.append(major_class)\n",
        "\n",
        "        # --- 2. Filter explanations only from the winning class ---\n",
        "        # Here we ensure that we only pass explanations associated with the winning class\n",
        "        # And we already perform a pre-check if the explanation is not null\n",
        "        explicacoes_candidatas = [\n",
        "            e for r, e in zip(results, explains)\n",
        "            if r == major_class and pd.notna(e)\n",
        "        ]\n",
        "\n",
        "        # --- 3. Select the best explanation (with semantic protection) ---\n",
        "        melhor_exp = get_best_explication_by_semantic_sim(explicacoes_candidatas)\n",
        "        final_explanations.append(melhor_exp)\n",
        "\n",
        "    # Add final columns to the dataframe\n",
        "    df[output_result_col] = final_results\n",
        "    df[output_explain_col] = final_explanations\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MnPY32Bg7p_v",
      "metadata": {
        "id": "MnPY32Bg7p_v"
      },
      "source": [
        "\n",
        "### Execution of Majority Voting Semantic Consensus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KnM8IS1EGEbb",
      "metadata": {
        "id": "KnM8IS1EGEbb"
      },
      "outputs": [],
      "source": [
        "# It uses majority voting and selects the best explanation based on semantic similarity.\n",
        "\n",
        "# A\n",
        "evaluator_a_final_dataset = apply_majority_voting(\n",
        "    df_evaluated_A_merged,\n",
        "    cols_result=['RESULT_A_1', 'RESULT_A_2', 'RESULT_A_3'],\n",
        "    cols_explain=['EXPLAIN_A_1', 'EXPLAIN_A_2', 'EXPLAIN_A_3'],\n",
        "    binary_options=['EXCLUDED', 'INCLUDED'],\n",
        "    output_result_col='FINAL_RESULT_A',\n",
        "    output_explain_col='FINAL_EXPLAIN_A'\n",
        ")\n",
        "\n",
        "# B\n",
        "evaluator_b_final_dataset = apply_majority_voting(\n",
        "    df_evaluated_B_merged,\n",
        "    cols_result=['RESULT_B_1', 'RESULT_B_2', 'RESULT_B_3'],\n",
        "    cols_explain=['EXPLAIN_B_1', 'EXPLAIN_B_2', 'EXPLAIN_B_3'],\n",
        "    binary_options=['EXCLUDED', 'INCLUDED'],\n",
        "    output_result_col='FINAL_RESULT_B',\n",
        "    output_explain_col='FINAL_EXPLAIN_B'\n",
        ")\n",
        "\n",
        "\n",
        "# # save datasets on drive\n",
        "# output_dir = \"/content/drive/MyDrive/PLN_DATASETS\"\n",
        "# output_filename_ = f\"rsl-evaluator-post-processed_a_final.csv\"\n",
        "# output_path_ = os.path.join(output_dir, output_filename_)\n",
        "\n",
        "# evaluator_a_final_dataset.to_csv(output_path_, index=False)\n",
        "# print(f\"File A saved: {output_path_}\")\n",
        "\n",
        "\n",
        "# output_dir = \"/content/drive/MyDrive/PLN_DATASETS\"\n",
        "# output_filename_ = f\"rsl-evaluator-post-processed_b_final.csv\"\n",
        "# output_path_ = os.path.join(output_dir, output_filename_)\n",
        "\n",
        "# evaluator_b_final_dataset.to_csv(output_path_, index=False)\n",
        "# print(f\"File B saved: {output_path_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbOeYGN8hOZ1"
      },
      "source": [
        "### Execution of Majority Voting Semantic Consensus on Judges"
      ],
      "id": "bbOeYGN8hOZ1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isYtBIWShOZ1"
      },
      "outputs": [],
      "source": [
        "# It uses majority voting of Judges.\n",
        "\n",
        "judge = apply_majority_voting(\n",
        "    judge_final_merged_to_metric,\n",
        "    cols_result=['RESULT_JUDGE_A', 'RESULT_JUDGE_A', 'RESULT_JUDGE_A'],\n",
        "    cols_explain=['EXPLAIN_JUDGEA', 'EXPLAIN_JUDGEB', 'EXPLAIN_JUDGEB'],\n",
        "    binary_options=['EXCLUDED', 'INCLUDED'],\n",
        "    output_result_col='FINAL_RESULT',\n",
        "    output_explain_col='FINAL_EXPLAIN'\n",
        ")\n"
      ],
      "id": "isYtBIWShOZ1"
    },
    {
      "cell_type": "code",
      "source": [
        "judge.columns\n",
        "\n",
        "# save datasets on drive\n",
        "# output_dir = \"/content/drive/MyDrive/PLN_DATASETS\"\n",
        "# output_filename_ = f\"rsl-judges-post-processed_final_decision.csv\"\n",
        "# output_path_ = os.path.join(output_dir, output_filename_)\n",
        "\n",
        "# judge.to_csv(output_path_, index=False)\n",
        "print(f\"Judges Consul saved: {output_path_}\")"
      ],
      "metadata": {
        "id": "QY8bjf3jjRuN"
      },
      "id": "QY8bjf3jjRuN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "C0kjYeV97r_2",
      "metadata": {
        "id": "C0kjYeV97r_2"
      },
      "source": [
        "## Judge"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CnKIgJNxoVPC",
      "metadata": {
        "id": "CnKIgJNxoVPC"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VCoWmt1hoUcE",
      "metadata": {
        "id": "VCoWmt1hoUcE"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import files\n",
        "from typing import List\n",
        "import os\n",
        "\n",
        "# allow Rust use paraellel on python\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "%pip install -q transformers accelerate\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from huggingface_hub import login\n",
        "login(hf_key)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xr7mDnAd93m7",
      "metadata": {
        "id": "xr7mDnAd93m7"
      },
      "source": [
        "### Import e Merge Evaluated Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zl9SCW6ZiYFY",
      "metadata": {
        "id": "Zl9SCW6ZiYFY"
      },
      "outputs": [],
      "source": [
        "url_a  = \"https://raw.githubusercontent.com/izichtl/small-language-models-on-systematic-reviews/refs/heads/main/PLN_FINAL_DATASETS/rsl-evaluator-post-processed_a_final.csv\"\n",
        "filename = \"rsl-evaluator-post-processed_a_final.csv\"\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    df_final_a = pd.read_csv(url_a)\n",
        "    df_final_a.to_csv(filename, index=False)\n",
        "else:\n",
        "    print('Imported from cache')\n",
        "    df_final_a = pd.read_csv(filename)\n",
        "\n",
        "url_b  = \"https://raw.githubusercontent.com/izichtl/small-language-models-on-systematic-reviews/refs/heads/main/PLN_FINAL_DATASETS/rsl-evaluator-post-processed_b_final.csv\"\n",
        "filename = \"rsl-evaluator-post-processed_b_final.csv\"\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    df_final_b = pd.read_csv(url_b)\n",
        "    df_final_b.to_csv(filename, index=False)\n",
        "else:\n",
        "    print('Imported from cache')\n",
        "    df_final_b = pd.read_csv(filename)\n",
        "\n",
        "print(len(df_final_a))\n",
        "print(len(df_final_b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ub8Af1uSjJ9u",
      "metadata": {
        "id": "ub8Af1uSjJ9u"
      },
      "outputs": [],
      "source": [
        "df_post_process_final_merged = df_final_a.merge(\n",
        "    df_final_b[['ID', 'RESULT_B_1',\n",
        "       'EXPLAIN_B_1', 'RESULT_B_2', 'EXPLAIN_B_2', 'RESULT_B_3', 'EXPLAIN_B_3',\n",
        "       'FINAL_RESULT_B', 'FINAL_EXPLAIN_B']],\n",
        "    on='ID',\n",
        "    how='left'\n",
        ")\n",
        "# df_post_process_final_merged.columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9_-kEbqkgxq",
      "metadata": {
        "id": "e9_-kEbqkgxq"
      },
      "source": [
        "### Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FfPExSGVnm8b",
      "metadata": {
        "id": "FfPExSGVnm8b"
      },
      "outputs": [],
      "source": [
        "judge_prompt_v1 = \"\"\"\"\n",
        "You are a scientific article evaluation judge. You will receive:\n",
        "- The title and abstract of a scientific article.\n",
        "- Two independent evaluations (Decision A and Decision B), each containing a classification\n",
        "  (\"INCLUDED\" or \"EXCLUDED\") and an explanation.\n",
        "\n",
        "Your task is to analyze the article critically and determine **which decision (A or B) better adheres\n",
        "to the inclusion and exclusion criteria**, and therefore is the most accurate.\n",
        "\n",
        "To perform the evaluation, follow this step-by-step reasoning:\n",
        "\n",
        "1. Check whether the article violates any Exclusion Criteria:\n",
        "• EC1. Is the article written in a language other than English?\n",
        "• EC3. Does the study not address software requirements?\n",
        "• EC4. Is the study incomplete, a draft, presentation slides, or just an abstract?\n",
        "• EC5. Is the study tertiary or a meta-analysis?\n",
        "• EC6. Does it deal with teaching agile methods?\n",
        "• EC7. Does it fail to address at least one agile methodology?\n",
        "• EC9. Does it fail to answer at least one research question?\n",
        "\n",
        "2. Confirm whether the article meets all Inclusion Criteria:\n",
        "• IC1. Does it address requirements in software projects using agile methodologies?\n",
        "• IC2. Is it a study from industry or academia?\n",
        "• IC3. Is it qualitative or quantitative research?\n",
        "• IC4. Is it a primary or secondary study?\n",
        "\n",
        "3. Evaluate Decision A and Decision B:\n",
        "• Check which decision correctly applies the exclusion criteria.\n",
        "• Check which decision correctly applies the inclusion criteria.\n",
        "• Check which explanation shows better reasoning aligned with the title and abstract.\n",
        "• Identify incorrect assumptions or mistakes in either A or B.\n",
        "• Select the decision that is more accurate and better justified.\n",
        "\n",
        "4. Respond ONLY with a single JSON object in the format:\n",
        "\n",
        "{{\n",
        "  \"best_decision\": \"INCLUDED\" or \"EXCLUDED\",\n",
        "  \"justification\": \"<brief explanation of why the chosen decision is more accurate>\"\n",
        "}}\n",
        "\n",
        "---\n",
        "Title: {0}\n",
        "\n",
        "Abstract: {1}\n",
        "\n",
        "Decision A:\n",
        "{2}\n",
        "Justification A:\n",
        "{3}\n",
        "\n",
        "Decision B:\n",
        "{4}\n",
        "Justification B:\n",
        "{5}\n",
        "\n",
        "Instruction:\n",
        "Respond ONLY with the JSON object specified above. No additional text or code blocks.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extrac fields Method"
      ],
      "metadata": {
        "id": "8KAsahyWloN-"
      },
      "id": "8KAsahyWloN-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oA-mBaVlmoQw",
      "metadata": {
        "id": "oA-mBaVlmoQw"
      },
      "outputs": [],
      "source": [
        "def extract_fields_v2(text):\n",
        "    result_match = re.search(r'\"best_decision\"\\s*:\\s*\"([^\"]*)\"', text)\n",
        "    explanation_match = re.search(r'\"justification\"\\s*:\\s*\"([^\"]*)\"', text)\n",
        "\n",
        "    return {\n",
        "        \"result\": result_match.group(1) if result_match else None,\n",
        "        \"explanation\": explanation_match.group(1) if explanation_match else None\n",
        "    }\n",
        "\n",
        "def avaliador_execute_pipeline_judge(\n",
        "    df, target_cols, prompt_template, pipe,\n",
        "    output_col_result=\"result\",\n",
        "    output_col_explain=\"explain\",\n",
        "    compare_col_a=\"FINAL_RESULT_A\",\n",
        "    compare_col_b=\"FINAL_RESULT_B\",\n",
        "    max_tokens=256\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluates line by line: if compare_col_a == compare_col_b => does not execute pipe (repeats result, explain=\"\")\n",
        "    Otherwise => executes pipe and extracts the fields from the generated text.\n",
        "    \"\"\"\n",
        "    df2 = df.copy(deep=True)\n",
        "\n",
        "    def safe_str(x):\n",
        "        return \"\" if pd.isna(x) else str(x)\n",
        "\n",
        "    resultados = []\n",
        "    explanations = []\n",
        "\n",
        "    for i, (idx, row) in enumerate(df2.iterrows()):\n",
        "        # Line-by-line comparison, handling NaN: consider equal only if both are NaN or values are equal\n",
        "        a = row.get(compare_col_a)\n",
        "        b = row.get(compare_col_b)\n",
        "\n",
        "        both_na = pd.isna(a) and pd.isna(b)\n",
        "        equal_vals = (a == b) if not (pd.isna(a) or pd.isna(b)) else False\n",
        "\n",
        "        if both_na or equal_vals:\n",
        "            # Repeats the result (from compare_col_a) and empty explanation\n",
        "            resultados.append(\"\" if pd.isna(a) else a)\n",
        "            explanations.append(\"\")\n",
        "            print(f\"[{i}] SKIP pipe — {compare_col_a} == {compare_col_b} -> result kept: {a}\")\n",
        "            continue\n",
        "\n",
        "        # If different -> execute pipeline\n",
        "        prompt = prompt_template.format(*[safe_str(row[c]) for c in target_cols])\n",
        "        print(f\"=== INSTANCE {i} (will run pipe) ===\")\n",
        "        messages = [{\"role\": \"user\", \"content\": f\"{prompt}\"}]\n",
        "        print(messages)\n",
        "\n",
        "        output = pipe(prompt)\n",
        "\n",
        "        # Extracting the generated text\n",
        "        s = None\n",
        "        # Try to extract the generated string safely\n",
        "        try:\n",
        "            # Support different pipe formats\n",
        "            if isinstance(output, list) and len(output) > 0 and 'generated_text' in output[0]:\n",
        "                s = output[0]['generated_text']\n",
        "            elif isinstance(output, dict) and 'generated_text' in output:\n",
        "                s = output['generated_text']\n",
        "            else:\n",
        "                # Fallback: stringify\n",
        "                s = str(output)\n",
        "        except Exception as e:\n",
        "            s = str(output)\n",
        "            print(\"Warning: could not get generated_text directly:\", e)\n",
        "\n",
        "        print(\"=== RESPONSE (raw) ===\")\n",
        "        # print(\"Raw output:\", s)\n",
        "        print('------------------')\n",
        "\n",
        "        parsed = extract_fields_v2(s)\n",
        "\n",
        "        # Check if both were found\n",
        "        if parsed[\"result\"] is not None and parsed[\"explanation\"] is not None:\n",
        "            resultados.append(parsed[\"result\"])\n",
        "            explanations.append(parsed[\"explanation\"])\n",
        "        else:\n",
        "            # If it fails, put the raw string in result and empty explanation\n",
        "            resultados.append(s)\n",
        "            explanations.append(\"\")\n",
        "\n",
        "        print(\"Parsed:\", parsed)\n",
        "        print('------------------')\n",
        "        print(\"=== RESPONSE-END ===\")\n",
        "\n",
        "    # Assign results to df2\n",
        "    df2[output_col_result] = resultados\n",
        "    df2[output_col_explain] = explanations\n",
        "\n",
        "    return df2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xcykr-4GnkUj",
      "metadata": {
        "id": "xcykr-4GnkUj"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JW7geBA_kgOg",
      "metadata": {
        "id": "JW7geBA_kgOg"
      },
      "outputs": [],
      "source": [
        "# control judges execution\n",
        "ENABLE_JUDGE_1 = False\n",
        "ENABLE_JUDGE_2 = False\n",
        "ENABLE_JUDGE_3 = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xLK_0KbEkgLr",
      "metadata": {
        "id": "xLK_0KbEkgLr"
      },
      "outputs": [],
      "source": [
        "if ENABLE_JUDGE_1:\n",
        "  judge_pipeline_J1 = pipeline(\n",
        "      task=\"text-generation\",\n",
        "      model=\"Unbabel/M-Prometheus-3B\",\n",
        "      device=0,\n",
        "      return_full_text=False,\n",
        "  )\n",
        "\n",
        "if ENABLE_JUDGE_2:\n",
        "  judge_pipeline_J2 = pipeline(\n",
        "      task=\"text-generation\",\n",
        "      model=\"GAIR/autoj-bilingual-6b\",\n",
        "      device=0,\n",
        "      return_full_text=False,\n",
        "  )\n",
        "\n",
        "if ENABLE_JUDGE_3:\n",
        "  judge_pipeline_J3 = pipeline(\n",
        "      task=\"text-generation\",\n",
        "      model=\"grounded-ai/phi3-hallucination-judge-merge\",\n",
        "      device=0,\n",
        "      return_full_text=False,\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Judges Execution"
      ],
      "metadata": {
        "id": "yP8AX-r0XXmU"
      },
      "id": "yP8AX-r0XXmU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wRbi-iVCkgIy",
      "metadata": {
        "id": "wRbi-iVCkgIy"
      },
      "outputs": [],
      "source": [
        "ITEM = \"A\"\n",
        "EXECUTION = '1'\n",
        "if ENABLE_JUDGE_1:\n",
        "\n",
        "    # output_dir = \"/content/drive/MyDrive/PLN_DATASETS\"\n",
        "    # import os\n",
        "    # os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    df_input = df_post_process_final_merged\n",
        "    print(\"Tamanho:\", len(df_input))\n",
        "\n",
        "    result_col = f\"RESULT_JUDGE_{ITEM}\"\n",
        "    explain_col = f\"EXPLAIN_JUDGE{ITEM}\"\n",
        "\n",
        "    df_result = avaliador_execute_pipeline_judge(\n",
        "        df=df_input,\n",
        "        target_cols=[\"TITLE\", \"ABSTRACT\", 'FINAL_RESULT_A','FINAL_EXPLAIN_A', 'FINAL_RESULT_B', 'FINAL_EXPLAIN_B'],\n",
        "        prompt_template=judge_prompt_v1,\n",
        "        pipe=judge_pipeline_J1,\n",
        "        output_col_result=result_col,\n",
        "        output_col_explain=explain_col,\n",
        "        compare_col_a=\"FINAL_RESULT_A\",\n",
        "        compare_col_b=\"FINAL_RESULT_B\"\n",
        "    )\n",
        "\n",
        "    output_filename = f\"rsl-judge-processed-dataset_{ITEM.lower()}_execution_{EXECUTION}.csv\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "    df_result.to_csv(output_path, index=False)\n",
        "    print(f\"✔ Arquivo salvo: {output_path}\")\n",
        "\n",
        "    print(\"\\nDone with successsss.\")\n",
        "\n",
        "\n",
        "ITEM = \"B\"\n",
        "EXECUTION = '1'\n",
        "if ENABLE_JUDGE_2:\n",
        "\n",
        "    # output_dir = \"/content/drive/MyDrive/PLN_DATASETS\"\n",
        "    # import os\n",
        "    # os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    df_input = df_post_process_final_merged\n",
        "    print(\"Tamanho:\", len(df_input))\n",
        "\n",
        "    result_col = f\"RESULT_JUDGE_{ITEM}\"\n",
        "    explain_col = f\"EXPLAIN_JUDGE{ITEM}\"\n",
        "\n",
        "    df_result = avaliador_execute_pipeline_judge(\n",
        "        df=df_input,\n",
        "        target_cols=[\"TITLE\", \"ABSTRACT\", 'FINAL_RESULT_A','FINAL_EXPLAIN_A', 'FINAL_RESULT_B', 'FINAL_EXPLAIN_B'],\n",
        "        prompt_template=judge_prompt_v1,\n",
        "        pipe=judge_pipeline_J2,\n",
        "        output_col_result=result_col,\n",
        "        output_col_explain=explain_col,\n",
        "        compare_col_a=\"FINAL_RESULT_A\",\n",
        "        compare_col_b=\"FINAL_RESULT_B\"\n",
        "    )\n",
        "\n",
        "    output_filename = f\"rsl-judge-processed-dataset_{ITEM.lower()}_execution_{EXECUTION}.csv\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "    df_result.to_csv(output_path, index=False)\n",
        "    print(f\"✔ Arquivo salvo: {output_path}\")\n",
        "\n",
        "    print(\"\\nDone with successsss.\")\n",
        "\n",
        "ITEM = \"C\"\n",
        "EXECUTION = '1'\n",
        "if ENABLE_JUDGE_3:\n",
        "\n",
        "    # output_dir = \"/content/drive/MyDrive/PLN_DATASETS\"\n",
        "    # import os\n",
        "    # os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    df_input = df_post_process_final_merged\n",
        "    print(\"Tamanho:\", len(df_input))\n",
        "\n",
        "    result_col = f\"RESULT_JUDGE_{ITEM}\"\n",
        "    explain_col = f\"EXPLAIN_JUDGE{ITEM}\"\n",
        "\n",
        "    df_result = avaliador_execute_pipeline_judge(\n",
        "        df=df_input,\n",
        "        target_cols=[\"TITLE\", \"ABSTRACT\", 'FINAL_RESULT_A','FINAL_EXPLAIN_A', 'FINAL_RESULT_B', 'FINAL_EXPLAIN_B'],\n",
        "        prompt_template=judge_prompt_v1,\n",
        "        pipe=judge_pipeline_J3,\n",
        "        output_col_result=result_col,\n",
        "        output_col_explain=explain_col,\n",
        "        compare_col_a=\"FINAL_RESULT_A\",\n",
        "        compare_col_b=\"FINAL_RESULT_B\"\n",
        "    )\n",
        "\n",
        "    output_filename = f\"rsl-judge-processed-dataset_{ITEM.lower()}_execution_{EXECUTION}.csv\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "    # df_result.to_csv(output_path, index=False)\n",
        "\n",
        "    print(\"\\nDone with successsss.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZvRXEegLNKD1",
      "metadata": {
        "id": "ZvRXEegLNKD1"
      },
      "source": [
        "## Evaluation of results\n",
        "\n",
        "- Can run without others cells"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import packages used in Evaluation"
      ],
      "metadata": {
        "id": "dzVGBiiRudHI"
      },
      "id": "dzVGBiiRudHI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KdZrnAR5PMCn",
      "metadata": {
        "id": "KdZrnAR5PMCn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# mont drive for salve processed datasets\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Evalutor A and B Datasets from github"
      ],
      "metadata": {
        "id": "3J-0QmZ1zwlA"
      },
      "id": "3J-0QmZ1zwlA"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "evaluator_final_dataset_a_url = 'https://raw.githubusercontent.com/izichtl/small-language-models-on-systematic-reviews/refs/heads/main/PLN_FINAL_DATASETS/rsl-evaluator-post-processed_a_final.csv'\n",
        "evaluator_final_dataset_b_url = 'https://raw.githubusercontent.com/izichtl/small-language-models-on-systematic-reviews/refs/heads/main/PLN_FINAL_DATASETS/rsl-evaluator-post-processed_b_final.csv'\n",
        "\n",
        "evaluator_final_dataset_a_filename = 'rsl-evaluator-post-processed_a_final.csv'\n",
        "evaluator_final_dataset_b_filename = 'rsl-evaluator-post-processed_b_final.csv'\n",
        "\n",
        "\n",
        "if not os.path.exists(evaluator_final_dataset_a_filename):\n",
        "    evaluator_final_dataset_a = pd.read_csv(evaluator_final_dataset_a_url)\n",
        "    evaluator_final_dataset_a.to_csv(evaluator_final_dataset_a_filename, index=False)\n",
        "else:\n",
        "    evaluator_final_dataset_a = pd.read_csv(evaluator_final_dataset_a_filename)\n",
        "\n",
        "if not os.path.exists(evaluator_final_dataset_b_filename):\n",
        "    evaluator_final_dataset_b = pd.read_csv(evaluator_final_dataset_b_url)\n",
        "    evaluator_final_dataset_b.to_csv(evaluator_final_dataset_b_filename, index=False)\n",
        "else:\n",
        "    evaluator_final_dataset_b = pd.read_csv(evaluator_final_dataset_b_filename)\n",
        "\n",
        "print(f\"Evaluator dataset a len: {len(evaluator_final_dataset_a)}\")\n",
        "print(f\"Evaluator dataset b len: {len(evaluator_final_dataset_b)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "D4sA2yvdyJkH"
      },
      "id": "D4sA2yvdyJkH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To merge into a final evaluator dataset"
      ],
      "metadata": {
        "id": "WaI6sSFPSSvz"
      },
      "id": "WaI6sSFPSSvz"
    },
    {
      "cell_type": "code",
      "source": [
        "# merge dataset to calc metric between\n",
        "evaluator_final_merged_to_metric = evaluator_final_dataset_a.merge(\n",
        "    evaluator_final_dataset_b[[\"ID\", 'RESULT_B_1','EXPLAIN_B_1', 'RESULT_B_2', 'EXPLAIN_B_2', 'RESULT_B_3', \"FINAL_RESULT_B\", \"FINAL_EXPLAIN_B\"]],\n",
        "    on=\"ID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "evaluator_final_merged_to_metric.columns"
      ],
      "metadata": {
        "id": "6ow0O7vDSRkm"
      },
      "id": "6ow0O7vDSRkm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Judged Datasets from github"
      ],
      "metadata": {
        "id": "mtN19LfNuWRr"
      },
      "id": "mtN19LfNuWRr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UMa-OD39kd_w",
      "metadata": {
        "id": "UMa-OD39kd_w"
      },
      "outputs": [],
      "source": [
        "judged_a_url = 'https://raw.githubusercontent.com/izichtl/small-language-models-on-systematic-reviews/refs/heads/main/PLN_JUDGED_DATASETS/rsl-judge-processed-dataset_a_execution_1.csv'\n",
        "judged_b_url = 'https://raw.githubusercontent.com/izichtl/small-language-models-on-systematic-reviews/refs/heads/main/PLN_JUDGED_DATASETS/rsl-judge-processed-dataset_b_execution_1.csv'\n",
        "judged_c_url = 'https://raw.githubusercontent.com/izichtl/small-language-models-on-systematic-reviews/refs/heads/main/PLN_JUDGED_DATASETS/rsl-judge-processed-dataset_c_execution_1.csv'\n",
        "\n",
        "judged_a_filename = 'rsl-judge-processed-dataset_a_execution_1.csv'\n",
        "judged_b_filename = 'rsl-judge-processed-dataset_b_execution_1.csv'\n",
        "judged_c_filename = 'rsl-judge-processed-dataset_c_execution_1.csv'\n",
        "\n",
        "if not os.path.exists(judged_a_filename):\n",
        "    df_judged_a = pd.read_csv(judged_a_url)\n",
        "    df_judged_a.to_csv(judged_a_filename, index=False)\n",
        "else:\n",
        "    df_judged_a = pd.read_csv(judged_a_filename)\n",
        "\n",
        "if not os.path.exists(judged_b_filename):\n",
        "    df_judged_b = pd.read_csv(judged_b_url)\n",
        "    df_judged_b.to_csv(judged_b_filename, index=False)\n",
        "else:\n",
        "    df_judged_b = pd.read_csv(judged_b_filename)\n",
        "\n",
        "if not os.path.exists(judged_c_filename):\n",
        "    df_judged_c = pd.read_csv(judged_c_url)\n",
        "    df_judged_c.to_csv(judged_c_filename, index=False)\n",
        "else:\n",
        "    df_judged_c = pd.read_csv(judged_c_filename)\n",
        "\n",
        "print(f\"Judged dataset a len: {len(df_judged_a)}\")\n",
        "print(f\"Judged dataset b len: {len(df_judged_b)}\")\n",
        "print(f\"Judged dataset c len: {len(df_judged_c)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To normalize judge responde"
      ],
      "metadata": {
        "id": "8jmzfsQMRVVw"
      },
      "id": "8jmzfsQMRVVw"
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_judge_labels(df, target_column):\n",
        "    \"\"\"\n",
        "    Standardizes the judgment column labels.\n",
        "    \"\"\"\n",
        "    # Create a copy to ensure we are modifying a specific instance and avoiding view warnings\n",
        "    df_processed = df.copy()\n",
        "    valid_labels = ['INCLUDED', 'EXCLUDED']\n",
        "\n",
        "    # Identify rows that have non-standard labels (neither INCLUDED nor EXCLUDED)\n",
        "    non_standard_mask = ~df_processed[target_column].isin(valid_labels)\n",
        "\n",
        "    # Update values\n",
        "    # Rationale: To avoid loss of evidence, studies mentioned by the judge in the response\n",
        "    # (which originally had non-standard labels) were added as 'INCLUDED'.\n",
        "    if non_standard_mask.sum() > 0:\n",
        "        df_processed.loc[non_standard_mask, target_column] = 'INCLUDED'\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "\n",
        "df_judged_a = standardize_judge_labels(df_judged_a, 'RESULT_JUDGE_A')\n",
        "df_judged_b = standardize_judge_labels(df_judged_b, 'RESULT_JUDGE_B')\n",
        "df_judged_c = standardize_judge_labels(df_judged_c, 'RESULT_JUDGE_C')\n",
        "\n",
        "print(f\"Judged dataset a len: {len(df_judged_a)}\")\n",
        "print(df_judged_a['RESULT_JUDGE_A'].value_counts())\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(f\"Judged dataset b len: {len(df_judged_b)}\")\n",
        "print(df_judged_b['RESULT_JUDGE_B'].value_counts())\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(f\"Judged dataset c len: {len(df_judged_c)}\")\n",
        "print(df_judged_c['RESULT_JUDGE_C'].value_counts())"
      ],
      "metadata": {
        "id": "7BaOMHBeQc63"
      },
      "id": "7BaOMHBeQc63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judge_final_merged_to_metric = df_judged_a.merge(\n",
        "    df_judged_b[[\"ID\", \"RESULT_JUDGE_B\", \"EXPLAIN_JUDGEB\"]],\n",
        "    on=\"ID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "judge_final_merged_to_metric = judge_final_merged_to_metric.merge(\n",
        "    df_judged_c[[\"ID\", \"RESULT_JUDGE_C\", \"EXPLAIN_JUDGEC\"]],\n",
        "    on=\"ID\",\n",
        "    how=\"left\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "7prKjKDGRbII"
      },
      "id": "7prKjKDGRbII",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Judges Final Decision Dataset"
      ],
      "metadata": {
        "id": "JUUhUMdNkBGV"
      },
      "id": "JUUhUMdNkBGV"
    },
    {
      "cell_type": "code",
      "source": [
        "judged_final_url = 'https://raw.githubusercontent.com/izichtl/small-language-models-on-systematic-reviews/refs/heads/main/PLN_JUDGED_DATASETS/rsl-judges-post-processed_final_decision.csv'\n",
        "\n",
        "\n",
        "judged_final_filename = 'rsl-judges-post-processed_final_decision.csv'\n",
        "\n",
        "\n",
        "if not os.path.exists(judged_final_filename):\n",
        "    judged_final = pd.read_csv(judged_final_url)\n",
        "    judged_final.to_csv(judged_final_filename, index=False)\n",
        "else:\n",
        "    judged_final = pd.read_csv(judged_final_filename)\n",
        "\n",
        "\n",
        "print(f\"Judged Final dataset a len: {len(judged_final)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZmagnNp6kAb_"
      },
      "id": "ZmagnNp6kAb_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the final dataset of project - RSL_SML_JUDGES"
      ],
      "metadata": {
        "id": "rMeKqpkhVLsg"
      },
      "id": "rMeKqpkhVLsg"
    },
    {
      "cell_type": "code",
      "source": [
        "final_project_dataset = evaluator_final_merged_to_metric.merge(\n",
        "    judge_final_merged_to_metric[[\"ID\", 'RESULT_JUDGE_A', 'EXPLAIN_JUDGEA', 'RESULT_JUDGE_B', 'EXPLAIN_JUDGEB', 'RESULT_JUDGE_C', 'EXPLAIN_JUDGEC']],\n",
        "    on=\"ID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "final_project_dataset = final_project_dataset.merge(\n",
        "    judged_final[[\"ID\", 'FINAL_RESULT', 'FINAL_EXPLAIN',]],\n",
        "    on=\"ID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "\n",
        "final_project_dataset = final_project_dataset.rename(columns={'EXPLAIN_JUDGEA': 'EXPLAIN_JUDGE_A'})\n",
        "final_project_dataset = final_project_dataset.rename(columns={'EXPLAIN_JUDGEB': 'EXPLAIN_JUDGE_B'})\n",
        "final_project_dataset = final_project_dataset.rename(columns={'EXPLAIN_JUDGEC': 'EXPLAIN_JUDGE_C'})\n",
        "final_project_dataset['STATUS'] = final_project_dataset['STATUS'].str.upper()\n",
        "final_project_dataset.columns\n",
        "\n",
        "# save datasets on drive\n",
        "output_dir = \"/content/drive/MyDrive/PLN_DATASETS\"\n",
        "output_filename_ = f\"slm-as-a-judge-on-rsl-dataset.csv\"\n",
        "output_path_ = os.path.join(output_dir, output_filename_)\n",
        "\n",
        "# final_project_dataset.to_csv(output_path_, index=False)\n",
        "print(f\"Judges Consul saved: {output_path_}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ovwtGjR0SfeK"
      },
      "id": "ovwtGjR0SfeK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Methods for calculating and plotting similarity metrics."
      ],
      "metadata": {
        "id": "jry3I5BAxgKY"
      },
      "id": "jry3I5BAxgKY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge all results into one dataset"
      ],
      "metadata": {
        "id": "sN6pSIs0GaU9"
      },
      "id": "sN6pSIs0GaU9"
    },
    {
      "cell_type": "code",
      "source": [
        "def consolidar_resultados(dados_entrada):\n",
        "    \"\"\"\n",
        "    Receives a single DataFrame or a list of DataFrames generated by the 'gerar_tabela_concordancia' function.\n",
        "    Returns a consolidated table formatted for publication.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Normalization: Ensures we are dealing with a list, even if only one item is provided\n",
        "    if isinstance(dados_entrada, pd.DataFrame):\n",
        "        lista_dfs = [dados_entrada]\n",
        "    elif isinstance(dados_entrada, list):\n",
        "        lista_dfs = dados_entrada\n",
        "    else:\n",
        "        raise ValueError(\"The input must be a DataFrame or a list of DataFrames.\")\n",
        "\n",
        "    linhas_finais = []\n",
        "\n",
        "    for df in lista_dfs:\n",
        "        # Extracts key rows from the original DF\n",
        "        df_fleiss = df[df['Métrica'] == \"Fleiss' Kappa (Geral)\"]\n",
        "        df_krip = df[df['Métrica'] == \"Krippendorff's Alpha (Geral)\"]\n",
        "        df_cohen_media = df[df['Métrica'] == \"Cohen's Kappa (Média)\"]\n",
        "\n",
        "        # Extracts only the paired rows (excludes average and general metrics)\n",
        "        # Identifies rows that contain \"Cohen\" but NOT \"Média\"\n",
        "        df_pares = df[\n",
        "            df['Métrica'].str.contains(\"Cohen\", na=False) &\n",
        "            ~df['Métrica'].str.contains(\"Média\", na=False)\n",
        "        ]\n",
        "\n",
        "        # Auxiliary variables for general values\n",
        "        val_fleiss = df_fleiss['Valor'].values[0] if not df_fleiss.empty else '--'\n",
        "        int_fleiss = df_fleiss['Interpretação'].values[0] if not df_fleiss.empty else '--'\n",
        "\n",
        "        val_krip = df_krip['Valor'].values[0] if not df_krip.empty else '--'\n",
        "        int_krip = df_krip['Interpretação'].values[0] if not df_krip.empty else '--'\n",
        "\n",
        "        val_cohen_med = df_cohen_media['Valor'].values[0] if not df_cohen_media.empty else '--'\n",
        "        int_cohen_med = df_cohen_media['Interpretação'].values[0] if not df_cohen_media.empty else '--'\n",
        "\n",
        "        # List to collect annotator names for this group (to create the compound model name)\n",
        "        anotadores_grupo = set()\n",
        "\n",
        "        # CASE 1: Only 1 pair (Simple binary comparison, e.g., Human x Model)\n",
        "        # In this case, general Fleiss and Krip apply to this single row.\n",
        "        eh_comparacao_binaria = len(df_pares) == 1\n",
        "\n",
        "        for _, row in df_pares.iterrows():\n",
        "            # Parse the name: \"Cohen's Kappa (B_1 vs B_2)\" -> \"B_1 x B_2\"\n",
        "            metrica_raw = row['Métrica']\n",
        "            # Get what's inside the parentheses\n",
        "            conteudo = metrica_raw.split('(')[1].split(')')[0]\n",
        "            partes = conteudo.split(' vs ')\n",
        "\n",
        "            nome_modelo = f\"{partes[0]} x {partes[1]}\"\n",
        "            anotadores_grupo.update(partes)\n",
        "\n",
        "            nova_linha = {\n",
        "                'Model': nome_modelo,\n",
        "                'Cohen Kappa': row['Valor'],\n",
        "                'Interp CK': row['Interpretação'],\n",
        "                # If binary, fill everything. If group, pairs do not have individual Fleiss/Krip in the standard output\n",
        "                'Fleiss Kappa': val_fleiss if eh_comparacao_binaria else '--',\n",
        "                'Interp FK': int_fleiss if eh_comparacao_binaria else '--',\n",
        "                'Krippendorf': val_krip if eh_comparacao_binaria else '--',\n",
        "                'Interp Krip': int_krip if eh_comparacao_binaria else '--'\n",
        "            }\n",
        "            linhas_finais.append(nova_linha)\n",
        "\n",
        "        # CASE 2: Group Comparison (>2 annotators)\n",
        "        # Adds a summary row at the end with the combination of all\n",
        "        if not eh_comparacao_binaria and len(anotadores_grupo) > 0:\n",
        "            # Creates compound name: \"B_1 x B_2 x B_3\"\n",
        "            nome_grupo = \" x \".join(sorted(list(anotadores_grupo)))\n",
        "\n",
        "            linha_resumo = {\n",
        "                'Model': nome_grupo,\n",
        "                'Cohen Kappa': val_cohen_med, # Average of Cohens\n",
        "                'Interp CK': int_cohen_med,\n",
        "                'Fleiss Kappa': val_fleiss,\n",
        "                'Interp FK': int_fleiss,\n",
        "                'Krippendorf': val_krip,\n",
        "                'Interp Krip': int_krip\n",
        "            }\n",
        "            linhas_finais.append(linha_resumo)\n",
        "\n",
        "    # Creates the final DataFrame\n",
        "    df_final = pd.DataFrame(linhas_finais)\n",
        "\n",
        "    # Reorder columns to ensure the requested layout\n",
        "    cols_order = [\n",
        "        'Model',\n",
        "        'Cohen Kappa', 'Interp CK',\n",
        "        'Fleiss Kappa', 'Interp FK',\n",
        "        'Krippendorf', 'Interp Krip'\n",
        "    ]\n",
        "\n",
        "    return df_final[cols_order]"
      ],
      "metadata": {
        "id": "N6INUrq-GZCb"
      },
      "id": "N6INUrq-GZCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calcule Metrics"
      ],
      "metadata": {
        "id": "grWuai73GgQm"
      },
      "id": "grWuai73GgQm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fWy_pKtjNLvb",
      "metadata": {
        "id": "fWy_pKtjNLvb"
      },
      "outputs": [],
      "source": [
        "def nominal_krippendorff_alpha(data):\n",
        "    units_cnt = len(data)\n",
        "    if units_cnt == 0: return 0\n",
        "\n",
        "    vals = np.unique(data)\n",
        "    val_map = {v: i for i, v in enumerate(vals)}\n",
        "    n_cat = len(vals)\n",
        "\n",
        "    matrix = np.vectorize(val_map.get)(data)\n",
        "    n_observers = matrix.shape[1]\n",
        "\n",
        "    coincidence_matrix = np.zeros((n_cat, n_cat))\n",
        "\n",
        "    for row in matrix:\n",
        "        counts = np.bincount(row, minlength=n_cat)\n",
        "        for i in range(n_cat):\n",
        "            for j in range(n_cat):\n",
        "                if i == j:\n",
        "                    coincidence_matrix[i, j] += counts[i] * (counts[i] - 1)\n",
        "                else:\n",
        "                    coincidence_matrix[i, j] += counts[i] * counts[j]\n",
        "\n",
        "    n_total_judgments = units_cnt * n_observers * (n_observers - 1)\n",
        "    if n_total_judgments == 0: return 0\n",
        "\n",
        "    sum_diag = np.trace(coincidence_matrix)\n",
        "    observed_agreement = sum_diag / n_total_judgments\n",
        "\n",
        "    row_sums = np.sum(coincidence_matrix, axis=1)\n",
        "    total_sum = np.sum(row_sums)\n",
        "    expected_agreement = np.sum(row_sums**2) / (total_sum**2)\n",
        "\n",
        "    if expected_agreement == 1: return 1.0\n",
        "\n",
        "    alpha = 1 - (1 - observed_agreement) / (1 - expected_agreement)\n",
        "    return alpha\n",
        "\n",
        "def gerar_tabela_concordancia(titulo, df, colunas_anotadores, labels_anotadores=None):\n",
        "    \"\"\"\n",
        "    Calcula Cohen (pares), Fleiss e Krippendorff.\n",
        "\n",
        "    Args:\n",
        "        titulo (str): Título da análise (usado apenas para log ou controle).\n",
        "        df (pd.DataFrame): DataFrame com os dados.\n",
        "        colunas_anotadores (list): Lista com o nome das colunas no DF (ex: ['col_a', 'col_b']).\n",
        "        labels_anotadores (list, opcional): Lista com os nomes de exibição (ex: ['Humano', 'Modelo']).\n",
        "                                            Deve ter o mesmo tamanho de colunas_anotadores.\n",
        "    \"\"\"\n",
        "\n",
        "    # Validação de tamanho\n",
        "    if len(colunas_anotadores) < 2:\n",
        "        return pd.DataFrame([{\"Erro\": \"Necessário no mínimo 2 colunas para comparação\"}])\n",
        "\n",
        "    # Se labels foram passados, verifica se batem com as colunas\n",
        "    if labels_anotadores is not None:\n",
        "        if len(labels_anotadores) != len(colunas_anotadores):\n",
        "            return pd.DataFrame([{\"Erro\": f\"Número de labels ({len(labels_anotadores)}) diferente do número de colunas ({len(colunas_anotadores)}).\" }])\n",
        "        # Cria mapa: Coluna -> Nome Bonito\n",
        "        mapa_nomes = dict(zip(colunas_anotadores, labels_anotadores))\n",
        "    else:\n",
        "        # Se não passar label, usa o nome da coluna mesmo\n",
        "        mapa_nomes = {col: col for col in colunas_anotadores}\n",
        "\n",
        "    # 1. Preparação dos dados\n",
        "    df_clean = df[colunas_anotadores].dropna().copy()\n",
        "\n",
        "    for col in colunas_anotadores:\n",
        "        df_clean[col] = df_clean[col].astype(str).str.upper().str.strip()\n",
        "\n",
        "    unique_labels = sorted(set(df_clean.values.ravel()))\n",
        "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "    df_encoded = df_clean.replace(label_map)\n",
        "    matrix = df_encoded.values\n",
        "\n",
        "    resultados = []\n",
        "\n",
        "    # 2. Cohen's Kappa (Pares Dinâmicos)\n",
        "    pares = list(combinations(colunas_anotadores, 2))\n",
        "\n",
        "    cohen_values = []\n",
        "\n",
        "    for a1, a2 in pares:\n",
        "        score = cohen_kappa_score(df_encoded[a1], df_encoded[a2])\n",
        "        cohen_values.append(score)\n",
        "\n",
        "        # AQUI O AJUSTE: Usa o mapa para pegar o nome bonito\n",
        "        nome_bonito_a1 = mapa_nomes[a1]\n",
        "        nome_bonito_a2 = mapa_nomes[a2]\n",
        "\n",
        "        resultados.append({\n",
        "            \"Métrica\": f\"Cohen's Kappa ({nome_bonito_a1} vs {nome_bonito_a2})\",\n",
        "            \"Valor\": score\n",
        "        })\n",
        "\n",
        "    # Média do Cohen\n",
        "    if len(cohen_values) > 0:\n",
        "        resultados.append({\n",
        "            \"Métrica\": \"Cohen's Kappa (Média)\",\n",
        "            \"Valor\": np.mean(cohen_values)\n",
        "        })\n",
        "\n",
        "    # 3. Fleiss' Kappa\n",
        "    agg_data, categories = aggregate_raters(matrix)\n",
        "    fleiss = fleiss_kappa(agg_data)\n",
        "    resultados.append({\n",
        "        \"Métrica\": \"Fleiss' Kappa (Geral)\",\n",
        "        \"Valor\": fleiss\n",
        "    })\n",
        "\n",
        "    # 4. Krippendorff's Alpha\n",
        "    k_alpha = nominal_krippendorff_alpha(matrix)\n",
        "    resultados.append({\n",
        "        \"Métrica\": \"Krippendorff's Alpha (Geral)\",\n",
        "        \"Valor\": k_alpha\n",
        "    })\n",
        "\n",
        "    # 5. Formatação Final\n",
        "    df_res = pd.DataFrame(resultados)\n",
        "\n",
        "    def classificar_metricas(row):\n",
        "        v = row['Valor']\n",
        "        m = row['Métrica']\n",
        "\n",
        "        # Se for Krippendorff, usa a escala específica (Krippendorff, 2004)\n",
        "        if \"Krippendorff\" in m:\n",
        "            if v < 0.667: return \"Não confiável\"\n",
        "            if v <= 0.800: return \"Aceitável\"\n",
        "            if v < 0.900: return \"Boa\"\n",
        "            if v > 0.900: return \"Excelente\"\n",
        "            return \"Confiável\"\n",
        "\n",
        "        # Para Cohen e Fleiss, usa Landis & Koch (1977)\n",
        "        else:\n",
        "            if v < 0: return \"Pobre\"\n",
        "            if v <= 0.2: return \"Leve\"\n",
        "            if v <= 0.4: return \"Razoável\"\n",
        "            if v <= 0.6: return \"Moderada\"\n",
        "            if v <= 0.8: return \"Substancial\"\n",
        "            return \"Quase Perfeita\"\n",
        "\n",
        "    df_res['Interpretação'] = df_res.apply(classificar_metricas, axis=1)\n",
        "    df_res['Valor'] = df_res['Valor'].round(3)\n",
        "\n",
        "    return df_res"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot on model table"
      ],
      "metadata": {
        "id": "iT1JpJbYGjv5"
      },
      "id": "iT1JpJbYGjv5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BluOFoo5Oxuq",
      "metadata": {
        "id": "BluOFoo5Oxuq"
      },
      "outputs": [],
      "source": [
        "def plot_tabela_concordancia(df, titulo):\n",
        "    # Figure configuration\n",
        "    fig, ax = plt.subplots(figsize=(8, 4)) # Adjustable size (width, height)\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Create the table\n",
        "    # cellLoc='center' centers the text\n",
        "    table = ax.table(cellText=df.values, colLabels=df.columns, loc='center', cellLoc='center')\n",
        "\n",
        "    # Styling\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(11) # Font size\n",
        "    table.scale(1.2, 1.8)  # Adjust spacing (Width, Height of cells)\n",
        "\n",
        "    # Iterate over cells to apply colors and bold text\n",
        "    for (row, col), cell in table.get_celld().items():\n",
        "        cell.set_edgecolor('black') # Simple black border\n",
        "        cell.set_linewidth(0.5)     # Thin thickness for article\n",
        "\n",
        "        # Header (Row 0)\n",
        "        if row == 0:\n",
        "            cell.set_text_props(weight='bold', color='white')\n",
        "            cell.set_facecolor('#333333') # Dark gray/black background\n",
        "\n",
        "        # Data Rows\n",
        "        else:\n",
        "            # Checks if it's a summary line to highlight\n",
        "            texto_metrica = df.iloc[row-1, 0]\n",
        "            if \"Média\" in texto_metrica or \"Geral\" in texto_metrica:\n",
        "                cell.set_facecolor('#e6e6e6') # Very light gray for highlighting\n",
        "                cell.set_text_props(weight='bold') # Bold in final results\n",
        "            else:\n",
        "                cell.set_facecolor('white')\n",
        "\n",
        "    # Plot Title\n",
        "    plt.title(titulo, weight='bold', size=14, pad=10)\n",
        "\n",
        "    # Final adjustment and save/show\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plote Paper Table"
      ],
      "metadata": {
        "id": "bftpIDgcGrC-"
      },
      "id": "bftpIDgcGrC-"
    },
    {
      "cell_type": "code",
      "source": [
        "def renderizar_tabela_imagem(df_consolidado, title):\n",
        "\n",
        "    df_plot = df_consolidado.copy()\n",
        "\n",
        "    # --- WIDTH CALCULATION ---\n",
        "    # Defines a fixed width for 'Model' (e.g., 0.35 = 35% of total width)\n",
        "    # Divides the remaining (0.65) by the number of other columns\n",
        "    largura_model = 0.30\n",
        "    n_outras_cols = len(df_plot.columns) - 1\n",
        "    largura_outras = (1.0 - largura_model) / n_outras_cols\n",
        "\n",
        "    # Creates the list of widths in the correct column order\n",
        "    larguras = []\n",
        "    for col in df_plot.columns:\n",
        "        if col == 'Model':\n",
        "            larguras.append(largura_model)\n",
        "        else:\n",
        "            larguras.append(largura_outras)\n",
        "\n",
        "    # Figure Configuration\n",
        "    altura_fig = 0.6 * len(df_plot) + 1.2\n",
        "    fig, ax = plt.subplots(figsize=(14, altura_fig))\n",
        "\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Table Creation passing colWidths\n",
        "    tabela = ax.table(\n",
        "        cellText=df_plot.values,\n",
        "        colLabels=df_plot.columns,\n",
        "        colWidths=larguras, # <--- HERE IS THE MAGIC\n",
        "        loc='center',\n",
        "        cellLoc='center',\n",
        "        bbox=[0, 0, 1, 1]\n",
        "    )\n",
        "\n",
        "    # --- Styling ---\n",
        "\n",
        "    tabela.auto_set_font_size(False)\n",
        "    tabela.set_fontsize(11)\n",
        "\n",
        "    for (row, col), cell in tabela.get_celld().items():\n",
        "        cell.set_height(0.1)\n",
        "\n",
        "        # Header (Row 0)\n",
        "        if row == 0:\n",
        "            cell.set_text_props(weight='bold', color='white')\n",
        "            cell.set_facecolor('#40466e')\n",
        "            cell.set_edgecolor('white')\n",
        "        else:\n",
        "            # Zebra Striping\n",
        "            if row % 2 == 0:\n",
        "                cell.set_facecolor('#f5f5f5')\n",
        "            else:\n",
        "                cell.set_facecolor('white')\n",
        "\n",
        "            cell.set_edgecolor('#dddddd')\n",
        "            cell.set_text_props(color='black')\n",
        "\n",
        "            # Conditional Highlighting (columns 2, 4, 6 are the Interpretation ones)\n",
        "            if col in [2, 4, 6]:\n",
        "                try:\n",
        "                    txt = cell.get_text().get_text()\n",
        "                    if txt in ['Quase Perfeita', 'Substancial']:\n",
        "                        cell.set_text_props(weight='bold', color='#2e7d32')\n",
        "                    elif txt in ['Moderada']:\n",
        "                        cell.set_text_props(color='#ef6c00')\n",
        "                    elif txt in ['Leve', 'Pobre']:\n",
        "                        cell.set_text_props(color='#c62828')\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    plt.title(title, fontsize=14, pad=10, weight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tu3k_6r7Gt16"
      },
      "id": "tu3k_6r7Gt16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution of similarity metrics of Evalutor A and B.\n",
        "\n",
        "- results here"
      ],
      "metadata": {
        "id": "TDjzsr1a0X6r"
      },
      "id": "TDjzsr1a0X6r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calc metric to each combination"
      ],
      "metadata": {
        "id": "mDCPsIKDG2um"
      },
      "id": "mDCPsIKDG2um"
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['RESULT_A_1', 'RESULT_A_2', 'RESULT_A_3']\n",
        "models_name = ['Qwen3_01', 'Qwen3_02', 'Qwen3_03']\n",
        "tabela_final_a = gerar_tabela_concordancia(\"Avaliador A Qwen\", final_project_dataset, cols, models_name)"
      ],
      "metadata": {
        "id": "p0Vj8g7T0DwU"
      },
      "id": "p0Vj8g7T0DwU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uf7Ix5SSOq3U",
      "metadata": {
        "id": "uf7Ix5SSOq3U"
      },
      "outputs": [],
      "source": [
        "cols = ['RESULT_B_1', 'RESULT_B_2', 'RESULT_B_3']\n",
        "models_name = ['Gemma3_01', 'Gemma3_02', 'Gemma3_03']\n",
        "tabela_final_b = gerar_tabela_concordancia(\"Avaliador B Gemma\", final_project_dataset, cols, models_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cols = ['FINAL_RESULT_A', 'FINAL_RESULT_B']\n",
        "models_name = ['Qwen3_01', 'Gemma3_01']\n",
        "tabela_final_a_x_b = gerar_tabela_concordancia(\"Avaliadores A x B\", final_project_dataset, cols, models_name)\n",
        "# plot_tabela_concordancia(tabela_final_a_x_b, \"Avaliadores A x B\")"
      ],
      "metadata": {
        "id": "xlb5XxHk1eNu"
      },
      "id": "xlb5XxHk1eNu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['RESULT_JUDGE_A', 'RESULT_JUDGE_B', 'RESULT_JUDGE_C']\n",
        "models_name = ['M-Prometheus', 'Autoj-6B', 'Phi3-halluc']\n",
        "judges_concor = gerar_tabela_concordancia(\"Avaliadores A x B\", final_project_dataset, cols, models_name)\n",
        "# plot_tabela_concordancia(tabela_final_a_x_b, \"Avaliadores A x B\")"
      ],
      "metadata": {
        "id": "lyNEBZQ8l4ya"
      },
      "id": "lyNEBZQ8l4ya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge Results and plot final table"
      ],
      "metadata": {
        "id": "BiKe4y9jG6Ye"
      },
      "id": "BiKe4y9jG6Ye"
    },
    {
      "cell_type": "code",
      "source": [
        "tabela_paper = consolidar_resultados([tabela_final_a, tabela_final_b, tabela_final_a_x_b])\n",
        "renderizar_tabela_imagem(tabela_paper, 'Concordância Avaliadores')"
      ],
      "metadata": {
        "id": "1RzP3kK_CbYA"
      },
      "id": "1RzP3kK_CbYA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tabela_paper = consolidar_resultados([judges_concor])\n",
        "renderizar_tabela_imagem(tabela_paper, 'Concordância Juizes')"
      ],
      "metadata": {
        "id": "rgeXy6KkmqUo"
      },
      "id": "rgeXy6KkmqUo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution of similarity metrics with ground truth.\n",
        "\n",
        "- results here"
      ],
      "metadata": {
        "id": "QJ2xVIdFJZcV"
      },
      "id": "QJ2xVIdFJZcV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calc metric to each combination"
      ],
      "metadata": {
        "id": "aKcGuT2jJZcW"
      },
      "id": "aKcGuT2jJZcW"
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['STATUS', 'FINAL_RESULT_A', ]\n",
        "models_name = ['Ground Truth', 'Qwen3 Consolidado']\n",
        "tabela_final_a_human = gerar_tabela_concordancia(\"Avaliador A Qwen\", final_project_dataset, cols, models_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "__Ib1GHyJZcW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "__Ib1GHyJZcW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqL3iOKSJZcW"
      },
      "outputs": [],
      "source": [
        "cols = ['STATUS', 'FINAL_RESULT_B', ]\n",
        "models_name = ['Ground Truth', 'Gemma3 Consolidado']\n",
        "tabela_final_b_human = gerar_tabela_concordancia(\"Avaliador B Gemma\", final_project_dataset, cols, models_name)"
      ],
      "id": "CqL3iOKSJZcW"
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['STATUS', 'RESULT_JUDGE_A', ]\n",
        "models_name = ['Ground Truth', '| A∩B + M-Prometheus |']\n",
        "tabela_final_j1_human = gerar_tabela_concordancia(\"Avaliador B Gemma\", final_project_dataset, cols, models_name)\n",
        "\n",
        "cols = ['STATUS', 'RESULT_JUDGE_B', ]\n",
        "models_name = ['Ground Truth', '| A∩B + Autoj-6B |']\n",
        "tabela_final_j2_human = gerar_tabela_concordancia(\"Avaliador B Gemma\", final_project_dataset, cols, models_name)\n",
        "\n",
        "cols = ['STATUS', 'RESULT_JUDGE_C', ]\n",
        "models_name = ['Ground Truth', '| A∩B + Phi3-halluc |']\n",
        "tabela_final_j3_human = gerar_tabela_concordancia(\"Avaliador B Gemma\", final_project_dataset, cols, models_name)"
      ],
      "metadata": {
        "id": "CbHEKNRIV8O_"
      },
      "id": "CbHEKNRIV8O_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['STATUS', 'FINAL_RESULT', ]\n",
        "models_name = ['Ground Truth', 'Consenso Final']\n",
        "final_decision = gerar_tabela_concordancia(\"\", final_project_dataset, cols, models_name)"
      ],
      "metadata": {
        "id": "SZrEDoyWk2lD"
      },
      "id": "SZrEDoyWk2lD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge Results and plot final table"
      ],
      "metadata": {
        "id": "H_ULRqy_JZcW"
      },
      "id": "H_ULRqy_JZcW"
    },
    {
      "cell_type": "code",
      "source": [
        "tabela_paper_evaluators_human = consolidar_resultados([\n",
        "    tabela_final_a_human,\n",
        "    tabela_final_b_human,\n",
        "    tabela_final_j1_human,\n",
        "    tabela_final_j2_human,\n",
        "    tabela_final_j3_human,\n",
        "    final_decision\n",
        "    ])\n",
        "tabela_paper_evaluators_human = tabela_paper_evaluators_human.drop(columns=['Fleiss Kappa', 'Interp FK'])\n",
        "renderizar_tabela_imagem(tabela_paper_evaluators_human, 'Concordância Avaliadores x Ground Truth')"
      ],
      "metadata": {
        "id": "oaH57lfKJZcW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "oaH57lfKJZcW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary Evaluation Metrics\n"
      ],
      "metadata": {
        "id": "1RiZUnXTtTHX"
      },
      "id": "1RiZUnXTtTHX"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Rendering Function  ---\n",
        "def plotar_tabela_zebrada(df_consolidado, title):\n",
        "    df_plot = df_consolidado.copy()\n",
        "\n",
        "    # --- WIDTH CALCULATION ---\n",
        "    # Defines a fixed width for 'Model' (e.g., 0.35 = 35% of total width)\n",
        "    largura_model = 0.30\n",
        "    n_outras_cols = len(df_plot.columns) - 1\n",
        "    # Avoid division by zero\n",
        "    largura_outras = (1.0 - largura_model) / n_outras_cols if n_outras_cols > 0 else 0.7\n",
        "\n",
        "    # Creates the list of widths\n",
        "    larguras = []\n",
        "    for col in df_plot.columns:\n",
        "        if col == 'Model':\n",
        "            larguras.append(largura_model)\n",
        "        else:\n",
        "            larguras.append(largura_outras)\n",
        "\n",
        "    # Figure Configuration\n",
        "    altura_fig = 0.5 * len(df_plot) + 1.0\n",
        "    fig, ax = plt.subplots(figsize=(10, altura_fig))\n",
        "\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Table Creation\n",
        "    tabela = ax.table(\n",
        "        cellText=df_plot.values,\n",
        "        colLabels=df_plot.columns,\n",
        "        colWidths=larguras,\n",
        "        loc='center',\n",
        "        cellLoc='center',\n",
        "        bbox=[0, 0, 1, 1]\n",
        "    )\n",
        "\n",
        "    tabela.auto_set_font_size(False)\n",
        "    tabela.set_fontsize(11)\n",
        "\n",
        "    for (row, col), cell in tabela.get_celld().items():\n",
        "        cell.set_height(0.1) # Row height\n",
        "\n",
        "        # Header (Row 0)\n",
        "        if row == 0:\n",
        "            cell.set_text_props(weight='bold', color='white')\n",
        "            cell.set_facecolor('#40466e') # Dark Blue\n",
        "            cell.set_edgecolor('white')\n",
        "\n",
        "        # Data Rows\n",
        "        else:\n",
        "            # Zebra Striping (Alternating colors)\n",
        "            if row % 2 == 0:\n",
        "                cell.set_facecolor('#f5f5f5') # Light gray\n",
        "            else:\n",
        "                cell.set_facecolor('white')\n",
        "\n",
        "            cell.set_edgecolor('#dddddd') # Soft border\n",
        "            cell.set_text_props(color='black')\n",
        "\n",
        "            # --- Highlighting Logic ---\n",
        "            try:\n",
        "                # Get the value from the first column (Model) of this row\n",
        "                texto_modelo = str(df_plot.iloc[row-1, 0])\n",
        "                if \"Média\" in texto_modelo or \"Acurácia\" in texto_modelo or \"Geral\" in texto_modelo:\n",
        "                     cell.set_text_props(weight='bold', color='black')\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    plt.title(title, fontsize=14, pad=10, weight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- Processing Function ---\n",
        "def processar_e_avaliar(df_dataset, ground_truth, pred_col, label_exibicao):\n",
        "\n",
        "    # Cleaning\n",
        "    df_clean = df_dataset[[ground_truth, pred_col]].dropna()\n",
        "    y_true = df_clean[ground_truth].astype(str)\n",
        "    y_pred = df_clean[pred_col].astype(str)\n",
        "\n",
        "    # --- Step A: Generate Data ---\n",
        "    report_dict = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    df_metrics = pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "    # Translate and rename\n",
        "    rename_map = {\n",
        "        'accuracy': 'Acurácia Geral',\n",
        "        'macro avg': 'Média Macro',\n",
        "        'weighted avg': 'Média Ponderada'\n",
        "    }\n",
        "    df_metrics = df_metrics.rename(index=rename_map)\n",
        "\n",
        "    # Reset index and rename to 'Model' to activate width logic\n",
        "    df_metrics = df_metrics.reset_index().rename(columns={'index': 'Model'})\n",
        "\n",
        "    # Rounding\n",
        "    cols_float = ['precision', 'recall', 'f1-score']\n",
        "    df_metrics[cols_float] = df_metrics[cols_float].round(3)\n",
        "    df_metrics['support'] = df_metrics['support'].astype(int)\n",
        "\n",
        "    # Rename final columns to Portuguese\n",
        "    df_metrics.columns = ['Model', 'Precisão', 'Recall', 'F1-Score', 'Suporte']\n",
        "\n",
        "    # --- Step B: Plot Table (WITH NEW NAME) ---\n",
        "    plotar_tabela_zebrada(df_metrics, f\"Métricas: {label_exibicao}\")\n",
        "\n",
        "    # --- Step C: Plot Confusion Matrix ---\n",
        "    classes = sorted(list(set(y_true.unique()) | set(y_pred.unique())))\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
        "\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.title(f'Matriz de Confusão: {label_exibicao}')\n",
        "    plt.ylabel('Real')\n",
        "    plt.xlabel('Predito')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LuVEJrcvwPug"
      },
      "id": "LuVEJrcvwPug",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration dictionary mapping internal result column names to display labels\n",
        "configuracao_execucao = {\n",
        "    'FINAL_RESULT_A': 'Qwen3-4B-Instruct',\n",
        "    'FINAL_RESULT_B': 'Gemma-3-4b-it',\n",
        "    'RESULT_JUDGE_A': 'M-Prometheus-3B',\n",
        "    'RESULT_JUDGE_B': 'Autoj-bilingual-6b',\n",
        "    'RESULT_JUDGE_C': 'Phi3-hallucination-judge',\n",
        "    'FINAL_RESULT':   'Consenso Final'\n",
        "}\n",
        "\n",
        "# Iterate through each configured model/judge and process its results\n",
        "for coluna, label in configuracao_execucao.items():\n",
        "    try:\n",
        "        # Call the function to calculate and plot binary evaluation metrics (classification report and confusion matrix)\n",
        "        # 'STATUS' is used as the ground truth, 'coluna' is the prediction column, and 'label' is for display purposes\n",
        "        processar_e_avaliar(final_project_dataset, 'STATUS', coluna, label)\n",
        "    except Exception as e:\n",
        "        # Catch and print any errors that occur during processing for a specific column\n",
        "        print(f\"Error processing {coluna}: {e}\")"
      ],
      "metadata": {
        "id": "TwTMTX94jzfI"
      },
      "id": "TwTMTX94jzfI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0cfe65a2",
        "VbBn18_UUZUo",
        "SEu6OSt-URNi",
        "6a034050",
        "92ec24ff",
        "ea70117f",
        "ad8d5a0d",
        "93989685",
        "2215420a",
        "c5713099",
        "de2f6ba8",
        "KuRZw0QEQRcM",
        "a65ac3db",
        "8d2a32ed",
        "5bfcb0f5",
        "b9721537",
        "c668940c",
        "e71b8ecf",
        "75f11c5a",
        "wQ6k54fcgyrh",
        "UIPjyTlW50GX",
        "vrYw9iNHGmdN",
        "vcPDXceTCiP8",
        "MnPY32Bg7p_v",
        "bbOeYGN8hOZ1",
        "e9_-kEbqkgxq",
        "8KAsahyWloN-",
        "xcykr-4GnkUj",
        "yP8AX-r0XXmU",
        "dzVGBiiRudHI",
        "WaI6sSFPSSvz",
        "mtN19LfNuWRr",
        "8jmzfsQMRVVw",
        "rMeKqpkhVLsg",
        "sN6pSIs0GaU9",
        "grWuai73GgQm",
        "iT1JpJbYGjv5",
        "bftpIDgcGrC-",
        "TDjzsr1a0X6r",
        "mDCPsIKDG2um",
        "BiKe4y9jG6Ye",
        "QJ2xVIdFJZcV",
        "aKcGuT2jJZcW",
        "H_ULRqy_JZcW",
        "1RiZUnXTtTHX"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}